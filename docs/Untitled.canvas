{
	"nodes":[
		{"id":"99ca08cd13b54764","type":"text","text":"Polynomials\n- Anything and everything can and will defined in relation to something other than itself\n- Anything and everything can and will defined in relation to something other than itself\n\neverything can be described as a polynominal function in its binary Quadratic Form\n\nreal vs actual || relation vs action || logic vs operation || observation vs experience ||\n\nsubject vs predicate  || object vs interface || logic vs operation || class vs type ||\n\nnoun vs verb  || pro-noun vs adjective || sentence vs statement ||\n\nThe x occurring in a polynomial is commonly called a variable or an indeterminate. When the polynomial is considered as an expression, x is a fixed symbol which does not have any value (its value is \"indeterminate\"). However, when one considers the function defined by the polynomial, then x represents the argument of the function, and is therefore called a \"variable\".or more fundamental to Polynomial functions are expressions of constants and symbols called variables or indeterminates as:\ndegrees, coefficients, exponents, and constants.\n\nPolynomials of small degree have been given specific names. A polynomial of degree zero is a constant polynomial, or simply a constant. Polynomials of degree one, two or three are respectively linear polynomials, quadratic polynomials and cubic polynomials For higher degrees, the specific names are not commonly used, although quartic polynomial (for degree four) and quintic polynomial (for degree five) are sometimes used. The names for the degrees may be applied to the polynomial or to its terms. \n\n# With homogeneous polynomial\n- we can apply \n \t- boolean logic truth table to a binomial, \n \t- trinary logic truth table trinomial, or \n \t- binomial factorization to extract a multinomials \n\t \t- homogeneous zero polynomial, or\n\t \t- homogeneous boolean logic truth proof, or \n\t \t- homogeneous boolean logic truth table, or\n\t \t- trinary logic truth table, or\n\t \t\nPolynomials can be classified by the number of terms with nonzero coefficients, so that a one-term polynomial is called a monomial, a two-term polynomial is called a binomial, and a three-term polynomial is called a trinomial. A polynomial with two or more terms is also called a multinomial\n\nIn the case of polynomials in more than one indeterminate, a polynomial is called homogeneous of 0 degree if all of its non-zero terms have degree n. The zero polynomial is homogeneous, and, as a homogeneous polynomial, its degree is undefined.\n\nA term with no indeterminates and a polynomial with no indeterminates are called, respectively, a constant term and a constant polynomial. The degree of a constant term and of a nonzero constant polynomial is 0. The degree of the zero polynomial 0 (which has no terms at all) is either left explicitly undefined, or defined as negative (either −1 or −∞).  The zero polynomial is also unique in that it is the only polynomial in one indeterminate that has an infinite number of roots. The graph of the zero polynomial, f(x)=0, is the x-axis.\n\n\n\n---\nTwo terms with the same indeterminates raised to the same powers are called \"similar terms\" or \"like terms\", and they can be combined, using the distributive law, into a single term whose coefficient is the sum of the coefficients of the terms that were combined. It may happen that this makes the coefficient \n0\n{\\displaystyle 0}.[12]\n\nA real polynomial is a polynomial with real coefficients. When it is used to define a function, the domain is not so restricted. However, a real polynomial function is a function from the reals to the reals that is defined by a real polynomial. Similarly, an integer polynomial is a polynomial with integer coefficients, and a complex polynomial is a polynomial with complex coefficients.\n\nA polynomial in one indeterminate is called a univariate polynomial, a polynomial in more than one indeterminate is called a multivariate polynomial.[15] A polynomial with two indeterminates is called a bivariate polynomial.[7] These notions refer more to the kind of polynomials one is generally working with than to individual polynomials; for instance, when working with univariate polynomials, one does not exclude constant polynomials (which may result from the subtraction of non-constant polynomials), although strictly speaking, constant polynomials do not contain any indeterminates at all. It is possible to further classify multivariate polynomials as bivariate, trivariate, and so on, according to the maximum number of indeterminates allowed. Again, so that the set of objects under consideration be closed under subtraction, a study of trivariate polynomials usually allows bivariate polynomials, and so on. It is also common to say simply \"polynomials in \nx\n,\ny\n{\\displaystyle x,y}, and \nz\n{\\displaystyle z}\", listing the indeterminates allowed.\n\nDivision\nThe division of one polynomial by another is not typically a polynomial. Instead, such ratios are a more general family of objects, called rational fractions, rational expressions, or rational functions, depending on context.[19] This is analogous to the fact that the ratio of two integers is a rational number, not necessarily an integer.[20][21] For example, the fraction \n1\n/\n(\nx\n2\n+\n1\n)\n{\\displaystyle 1/(x^{2}+1)} is not a polynomial, and it cannot be written as a finite sum of powers of the variable \nx\n{\\displaystyle x}.\n\nFor polynomials in one variable, there is a notion of Euclidean division of polynomials, generalizing the Euclidean division of integers.[e] This notion of the division \na\n(\nx\n)\n/\nb\n(\nx\n)\n{\\displaystyle a(x)/b(x)} results in two polynomials, a quotient \nq\n(\nx\n)\n{\\displaystyle q(x)} and a remainder \nr\n(\nx\n)\n{\\displaystyle r(x)}, such that \na\n=\nb\nq\n+\nr\n{\\displaystyle a=bq+r} and \ndeg\n⁡\n(\nr\n)\n<\ndeg\n⁡\n(\nb\n)\n{\\displaystyle \\deg(r)<\\deg(b)}, where \ndeg\n⁡\n(\np\n)\n{\\displaystyle \\deg(p)} is the degree of \np\n{\\displaystyle p}. The quotient and remainder may be computed by any of several algorithms, including polynomial long division and synthetic division.[22]\n\nWhen the denominator \nb\n(\nx\n)\n{\\displaystyle b(x)} is monic and linear, that is, \nb\n(\nx\n)\n=\nx\n−\nc\n{\\displaystyle b(x)=x-c} for some constant \nc\n{\\displaystyle c}, then the polynomial remainder theorem asserts that the remainder of the division of \na\n(\nx\n)\n{\\displaystyle a(x)} by \nb\n(\nx\n)\n{\\displaystyle b(x)} is the evaluation \na\n(\nc\n)\n{\\displaystyle a(c)}.[21] In this case, the quotient may be computed by Ruffini's rule, a special case of synthetic division.[23]","x":-380,"y":-280,"width":443,"height":620},
		{"id":"651266e5ba4890ce","type":"text","text":"Graphs\n---\n=== Graphs ===\n<div class=\"floatright\">\n<gallery perrow=\"2\" widths=\"120px\" heights=\"120px\">\nFile:Algebra1 fnz fig037 pc.svg|Polynomial of degree 0:<br/><small>{{math|''f''(''x'') {{=}} 2}}</small>\nFile:Fonction de Sophie Germain.png|Polynomial of degree 1:<br/><small>{{math|''f''(''x'') {{=}} 2''x'' + 1}}</small>\nFile:Polynomialdeg2.svg|Polynomial of degree 2:<br/><small>{{math|''f''(''x'') {{=}} ''x''<sup>2</sup> − ''x'' − 2}}<br/>{{math|{{=}} (''x'' + 1)(''x'' − 2)}}</small>\nFile:Polynomialdeg3.svg|Polynomial of degree 3:<br/><small>{{math|''f''(''x'') {{=}} ''x''<sup>3</sup>/4 + 3''x''<sup>2</sup>/4 − 3''x''/2 − 2}}<br/>{{math|{{=}} 1/4 (''x'' + 4)(''x'' + 1)(''x'' − 2)}}</small>\nFile:Polynomialdeg4.svg|Polynomial of degree 4:<br/><small>{{math|''f''(''x'') {{=}} 1/14 (''x'' + 4)(''x'' + 1)(''x'' − 1)(''x'' − 3) <br/>+ 0.5}}</small>\nFile:Quintic polynomial.svg|Polynomial of degree 5:<br/><small>{{math|''f''(''x'') {{=}} 1/20 (''x'' + 4)(''x'' + 2)(''x'' + 1)(''x'' − 1)<br/>(''x'' − 3) + 2}}</small>\nFile:Sextic Graph.svg|Polynomial of degree 6:<br/><small>{{math|''f''(''x'') {{=}} 1/100 (''x''<sup>6</sup> − 2''x'' <sup>5</sup> − 26''x''<sup>4</sup> + 28''x''<sup>3</sup>}}<br/>{{math|+ 145''x''<sup>2</sup> − 26''x'' − 80)}}</small>\nFile:Septic graph.svg|Polynomial of degree 7:<br/><small>{{math|''f''(''x'') {{=}} (''x'' − 3)(''x'' − 2)(''x'' − 1)(''x'')(''x'' + 1)(''x'' + 2)}}<br/>{{math|(''x'' + 3)}}</small>\n</gallery>\n</div>\nA polynomial function in one real variable can be represented by a [[graph of a function|graph]].\n<ul>\n<li>\nThe graph of the zero polynomial\n{{block indent|{{math|1=''f''(''x'') = 0}}}} is the {{math|''x''}}-axis.\n</li>\n<li>\nThe graph of a degree 0 polynomial\n{{block indent|{{math|1=''f''(''x'') = ''a''<sub>0</sub>}}, where {{math|''a''<sub>0</sub> ≠ 0}},}} is a horizontal line with {{nowrap|{{math|''y''}}-intercept {{math|''a''<sub>0</sub>}}}}\n</li>\n<li>\nThe graph of a degree 1 polynomial (or linear function)\n{{block indent|{{math|1=''f''(''x'') = ''a''<sub>0</sub> + ''a''<sub>1</sub>''x''}}, where {{math|''a''<sub>1</sub> ≠ 0}},}} is an oblique line with {{nowrap|{{math|''y''}}-intercept {{math|''a''<sub>0</sub>}}}} and [[slope]] {{math|''a''<sub>1</sub>}}.\n</li>\n<li>\nThe graph of a degree 2 polynomial\n{{block indent|{{math|1=''f''(''x'') = ''a''<sub>0</sub> + ''a''<sub>1</sub>''x'' + ''a''<sub>2</sub>''x''<sup>2</sup>}}, where {{math|''a''<sub>2</sub> ≠ 0}}}} is a [[parabola]].\n</li>\n<li>\nThe graph of a degree 3 polynomial\n{{block indent|{{math|1=''f''(''x'') = ''a''<sub>0</sub> + ''a''<sub>1</sub>''x'' + ''a''<sub>2</sub>''x''<sup>2</sup> + ''a''<sub>3</sub>''x''<sup>3</sup>}}, where {{math|''a''<sub>3</sub> ≠ 0}}}} is a [[cubic equation|cubic curve]].\n</li>\n<li>\nThe graph of any polynomial with degree 2 or greater\n{{block indent|{{math|1=''f''(''x'') = ''a''<sub>0</sub> + ''a''<sub>1</sub>''x'' + ''a''<sub>2</sub>''x''<sup>2</sup> + ⋯ + ''a''<sub>''n''</sub>''x''<sup>''n''</sup>}}, where {{math|''a''<sub>''n''</sub> ≠ 0 and ''n'' ≥ 2}}}} is a continuous non-linear curve.\n</li>\n</ul>\n\nA non-constant polynomial function [[infinity#Calculus|tends to infinity]] when the variable increases indefinitely (in [[absolute value]]). If the degree is higher than one, the graph does not have any [[asymptote]]. It has two [[parabolic branch]]es with vertical direction (one branch for positive {{math|''x''}} and one for negative {{math|''x''}}).\n\nPolynomial graphs are analyzed in calculus using intercepts, slopes, concavity, and end behavior.\n","x":149,"y":-46,"width":250,"height":60},
		{"id":"d2a4fe8a07aeb41a","type":"text","text":"Keywords\n---\nWhen considering equations, the indeterminates (variables) of polynomials are also called [unknowns](https://en.wikipedia.org/wiki/Variable_\\(mathematics\\) \"Variable (mathematics)\"), and the _solutions_ are the possible values of the unknowns for which the equality is true (in general more than one solution may exist).","x":-19,"y":-402,"width":418,"height":202},
		{"id":"6983a6011f062d9f","type":"text","text":"Matrix polynomials\n---\n=== Matrix polynomials ===\n{{Main|Matrix polynomial}}\nA [[matrix polynomial]] is a polynomial with [[square matrix|square matrices]] as variables.<ref>{{cite book |title=Matrix Polynomials |volume=58 |series=Classics in Applied Mathematics |first1=Israel |last1=Gohberg |first2=Peter |last2=Lancaster |first3=Leiba |last3=Rodman |publisher=[[Society for Industrial and Applied Mathematics]] |location=Lancaster, PA |year=2009 |orig-year=1982 |isbn=978-0-89871-681-8 |zbl=1170.15300}}</ref> Given an ordinary, scalar-valued polynomial\n<math display=\"block\">P(x) = \\sum_{i=0}^n{ a_i x^i} =a_0 + a_1 x+ a_2 x^2 + \\cdots + a_n x^n, </math>\nthis polynomial evaluated at a matrix ''A'' is\n<math display=\"block\">P(A) = \\sum_{i=0}^n{ a_i A^i} =a_0 I + a_1 A + a_2 A^2 + \\cdots + a_n A^n,</math>\nwhere ''I'' is the [[identity matrix]].{{sfn|Horn|Johnson|1990|p=36}}\n\nA '''matrix polynomial equation''' is an equality between two matrix polynomials, which holds for the specific matrices in question. A '''matrix polynomial identity''' is a matrix polynomial equation which holds for all matrices ''A'' in a specified [[matrix ring]] ''M<sub>n</sub>''(''R'').\n","x":60,"y":220,"width":250,"height":720},
		{"id":"385328f9840bd8a3","type":"text","text":"Describe Matrices","x":25,"y":106,"width":250,"height":60},
		{"id":"27b99b49b76064d5","type":"text","text":"Fano Lottery\n---\n","x":-402,"y":771,"width":250,"height":60},
		{"id":"20eb5976e2e67fa8","type":"text","text":"Rational functions\n---\n### Rational functions\n\nMain article: [Rational function](https://en.wikipedia.org/wiki/Rational_function \"Rational function\")\n\nA [rational fraction](https://en.wikipedia.org/wiki/Rational_fraction \"Rational fraction\") is the [quotient](https://en.wikipedia.org/wiki/Quotient \"Quotient\") ([algebraic fraction](https://en.wikipedia.org/wiki/Algebraic_fraction \"Algebraic fraction\")) of two polynomials. Any [algebraic expression](https://en.wikipedia.org/wiki/Algebraic_expression \"Algebraic expression\") that can be rewritten as a rational fraction is a [rational function](https://en.wikipedia.org/wiki/Rational_function \"Rational function\").\n\nWhile polynomial functions are defined for all values of the variables, a rational function is defined only for the values of the variables for which the denominator is not zero.\n\nThe rational fractions include the Laurent polynomials, but do not limit denominators to powers of an indeterminate.","x":-408,"y":860,"width":250,"height":380},
		{"id":"2ff376873e68f445","type":"text","text":"Back Prpoagation\n---\n","x":-408,"y":1300,"width":250,"height":60},
		{"id":"40cf02008ea2c028","type":"text","text":"Laurent polynomials\n---\n=== Laurent polynomials ===\n{{Main|Laurent polynomial}}\n[[Laurent polynomial]]s are like polynomials, but allow negative powers of the variable(s) to occur.\n","x":-437,"y":1388,"width":250,"height":60},
		{"id":"6ae3793bad7591ef","type":"text","text":"Block Design\n---\n== Definition ==\nAn ''integral domain'' is a [[zero ring|nonzero]] [[commutative ring]] in which the product of any two nonzero elements is nonzero.  Equivalently:\n* An integral domain is a nonzero commutative ring with no nonzero [[zero divisor]]s.\n* An integral domain is a commutative ring in which the [[zero ideal]] {0} is a [[prime ideal]].\n* An integral domain is a nonzero commutative ring for which every nonzero element is [[cancellation property|cancellable]] under multiplication.\n* An integral domain is a ring for which the set of nonzero elements is a commutative [[monoid]] under multiplication (because a monoid must be [[closure (mathematics)| closed]] under multiplication).\n* An integral domain is a nonzero commutative ring in which for every nonzero element ''r'', the function that maps each element ''x'' of the ring to the product ''xr'' is [[injective]]. Elements ''r'' with this property are called ''regular'', so it is equivalent to require that every nonzero element of the ring be regular.\n* An integral domain is a ring that is [[isomorphic]] to a [[subring]] of a [[field (mathematics)|field]].  (Given an integral domain, one can embed it in its [[field of fractions]].)\n","x":-16,"y":1109,"width":250,"height":339},
		{"id":"cbd7a7360adcfa98","type":"text","text":"Y-Combinator\n---\nInstances of the zero object include, but are not limited to the following:\n\n- As a [group](https://en.wikipedia.org/wiki/Group_\\(mathematics\\) \"Group (mathematics)\"), the **zero group** or **[trivial group](https://en.wikipedia.org/wiki/Trivial_group \"Trivial group\")**.\n- As a [ring](https://en.wikipedia.org/wiki/Ring_\\(mathematics\\) \"Ring (mathematics)\"), the **[zero ring](https://en.wikipedia.org/wiki/Zero_ring \"Zero ring\")** or **trivial ring**.\n- As an [algebra over a field](https://en.wikipedia.org/wiki/Algebra_over_a_field \"Algebra over a field\") or [algebra over a ring](https://en.wikipedia.org/wiki/Algebra_over_a_ring \"Algebra over a ring\"), the **trivial algebra**.\n- As a [module](https://en.wikipedia.org/wiki/Module_\\(mathematics\\) \"Module (mathematics)\") (over a [ring](https://en.wikipedia.org/wiki/Ring_\\(algebra\\) \"Ring (algebra)\") R), the **zero module**. The term **trivial module** is also used, although it may be ambiguous, as a _trivial G-module_ is a [_G_-module](https://en.wikipedia.org/wiki/G-module \"G-module\") with a trivial action.\n- As a [vector space](https://en.wikipedia.org/wiki/Vector_space \"Vector space\") (over a [field](https://en.wikipedia.org/wiki/Field_\\(mathematics\\) \"Field (mathematics)\") R), the **zero vector space**, **zero-dimensional vector space** or just **zero space**.\n\nThese objects are described jointly not only based on the common singleton and trivial group structure, but also because of [shared category-theoretical properties](https://en.wikipedia.org/wiki/Zero_object_\\(algebra\\)#Properties).\n\n---\n\n### Zero Polynomial Function\n\nA zero polynomial function is of the form f(x) = 0, yes, it just contains just 0 and no other term or variable. Since f(x) = a constant here, it is a [constant function](https://www.cuemath.com/calculus/constant-functions/).","x":-994,"y":-120,"width":250,"height":320},
		{"id":"64c24f74c8c0740b","type":"text","text":"### Linear Polynomial Function\n---\nA linear polynomial function has a degree 1. It is of the form f(x) = ax + b. Some examples of a linear polynomial function are f(x) = x + 3, f(x) = 25x + 4, and f(y) = 8y – 3.","x":-1274,"y":-120,"width":250,"height":320},
		{"id":"f9275a4ea429aca1","type":"text","text":"Perceptron\n---\n##   \nHow to Determine a Polynomial Function?\n\nIn order to determine if a function is polynomial or not, the function needs to be checked against certain conditions for the exponents of the variables. These conditions are as follows:\n\n- The exponent of the variable in the function in every term must only be a non-negative whole number.  \n    i.e., the exponent of the variable should not be a fraction or negative number.\n- The variable of the function should not be inside a radical i.e, it should not contain any square roots, cube roots, etc.\n- The variable should not be in the [denominator](https://www.cuemath.com/numbers/denominator/).\n\nThe below-given table shows an example and some non-examples of polynomial functions:\n\n|Functions|Variable|Exponent|Polynomial Function or Not?|\n|---|---|---|---|\n|f(b) = 4b2 – 6b + b3 – 15|b|2 in b2;  <br>3 in b3|Yes|\n|f(x) = x2/3+ 2x|x|2/3 in x2/3 ;  <br>1 in 2x|No|\n|f(y) = 1/y3|y|-3 in 1/y3|No|\n\n**Note:** Remember that coefficients can be fractions, negative numbers, 0, or positive numbers. We just need to take care of the exponents of variables to determine whether it is a polynomial function.","x":-994,"y":320,"width":250,"height":290},
		{"id":"23c64bc3bdbe9b14","type":"text","text":"  \nTypes of Polynomial Functions\n---\nThe name of a polynomial is determined by the number of terms in it. The three most common polynomials we usually encounter are monomials, binomials, and trinomials.\n\n- [Monomials](https://www.cuemath.com/algebra/monomial/) are polynomials that contain only one term. Examples: 15x2, 3b, and 12y4\n- [Binomials](https://www.cuemath.com/algebra/binomial/) are polynomials that contain only two terms. Examples: x + y, 4x – 7, and 9x + 2\n- [Trinomials](https://www.cuemath.com/algebra/trinomial/) are polynomials that contain only three terms. Examples: x3 – 3 + 5x, z4 + 45 + 3z, and x2 – 12x + 15\n\nFurther, the polynomials are also classified based on their degrees. The four most common types of polynomials that are used in precalculus and algebra are zero polynomial function, linear polynomial function, quadratic polynomial function, and cubic polynomial function.","x":-1080,"y":1080,"width":250,"height":308},
		{"id":"3a954dae48f9f2b9","type":"text","text":"### Cubic Polynomial Function\n---\nA cubic polynomial function has a degree 3. It is of the form f(x) = ax3 + bx2 + cx + d. Some examples of a cubic polynomial function are f(y) = 4y3, f(y) = 15y3 – y2 + 10, and f(a) = 3a + a3.","x":-869,"y":801,"width":250,"height":180},
		{"id":"e668ac1d4c914e71","type":"text","text":"### Quadratic Polynomial Function\n---\nA quadratic polynomial function has a degree 2. It is of the form f(x) = ax2 + bx + c. Some examples of a quadratic polynomial function are f(m) = 5m2 – 12m + 4, f(x) = 14x2 – 6, and f(x) = x2 + 4x.","x":-1205,"y":796,"width":250,"height":191},
		{"id":"cf4dae9e161473eb","type":"text","text":"## Zeros of Polynomial Function\n---\nThe zeros (which are also known as roots or [x-intercepts](https://www.cuemath.com/geometry/x-intercept/)) of a polynomial function f(x) are numbers that satisfy the equation f(x) = 0. So to find the zeros of a polynomial function f(x):\n\n- Set f(x) = 0\n- Solve the equation using [solving techniques of equations](https://www.cuemath.com/algebra/solving-an-equation/).\n\n### Zeros of Linear Polynomial Function\n\nConsider a linear polynomial function f(x) = 16x - 4. To find its zeros:\n\n- Set f(x) = 0  \n    16x - 4 = 0\n- Solve it.  \n    16x = 4  \n    x = 1/4\n\nThus, the zero of f(x) is 1/4.\n","x":-1624,"y":541,"width":250,"height":399},
		{"id":"1fc5bf82d0f7ef54","type":"text","text":"Exponential polynomials\n---\n=== Exponential polynomials ===\nA bivariate polynomial where the second variable is substituted for an exponential function applied to the first variable, for example {{math|''P''(''x'', ''e''<sup>''x''</sup>)}}, may be called an [[exponential polynomial]].\n","x":-1624,"y":274,"width":250,"height":191},
		{"id":"acc014afcd745a1d","type":"text","text":"## Polynomial Function Graph\n---\nWe can represent all the polynomial functions in the form of a graph. Remember that the [domain](https://www.cuemath.com/calculus/domain-and-range-of-a-function/) of any polynomial function is the set of all [real numbers](https://www.cuemath.com/numbers/real-numbers/).\n\n### Graphing Polynomial Functions\n\nTo graph a simple polynomial function, we usually make a table of values with some random values of x and the corresponding values of f(x). Then we plot the points from the table and join them by a curve. Let us draw the graph for the quadratic polynomial function f(x) = x2.","x":-2380,"y":318,"width":250,"height":305},
		{"id":"09c1f8256aca96bf","type":"text","text":"### Zeros of Quadratic Polynomial Function\n---\n##   \nZeros of Polynomial Function\n\nThe zeros (which are also known as roots or [x-intercepts](https://www.cuemath.com/geometry/x-intercept/)) of a polynomial function f(x) are numbers that satisfy the equation f(x) = 0. So to find the zeros of a polynomial function f(x):\n\n- Set f(x) = 0\n- Solve the equation using [solving techniques of equations](https://www.cuemath.com/algebra/solving-an-equation/).\n\n### Zeros of Linear Polynomial Function\n\nConsider a linear polynomial function f(x) = 16x - 4. To find its zeros:\n\n- Set f(x) = 0  \n    16x - 4 = 0\n- Solve it.  \n    16x = 4  \n    x = 1/4\n\nThus, the zero of f(x) is 1/4.\n\n### Zeros of Quadratic Polynomial Function\n\nConsider a quadratic polynomial function f(x) = x2 + 2x - 5. To find its [zeros](https://www.cuemath.com/algebra/zeros-of-quadratic-polynomial/):\n\n- Set f(x) = 0.  \n    Then x2 + 2x - 5 = 0.\n- Solve it.  \n    Here a = 1, b = 2 and c = -5.  \n    Let us use the [quadratic formula](https://www.cuemath.com/algebra/quadratic-equations/) to find the quadratic roots, x = [-b ± √(b2 - 4ac)]/2a  \n    x = [-2 ± √(22 - 4(1)(-5))]/(2)(1)  \n    = [-2 ± √(4+20)]/2  \n    = [-2 ± √(24)]/2  \n    = [-2 ± 2√6]/2  \n    = -1 ± √6\n\nHence, -1 + √6 and -1 -√6 are the zeros of the polynomial function f(x). Remember that the [irrational](https://www.cuemath.com/numbers/irrational-numbers/) roots and [complex](https://www.cuemath.com/numbers/complex-numbers/) roots of a polynomial function always occur in pairs.\n\n### Zeros of Cubic Polynomial Function\n\nFinding the [zeros of cubic polynomials](https://www.cuemath.com/algebra/zeros-of-a-cubic-polynomial/) is same as that of quadratic equations. But to make it to a much simpler form, we can use some of these special products:\n\n- [Perfect cube](https://www.cuemath.com/a-plus-b-cube-formula/) (2 forms): a3 ± 3a2b + 3ab2 ± b3 = (a ± b)3\n- [Difference of the cubes](https://www.cuemath.com/difference-of-cubes-formula/): a3 − b3 = (a − b)(a2 + ab + b2)\n- [Sum of the cubes](https://www.cuemath.com/sum-of-cubes-formula/): a3 + b3 = (a + b)(a2 − ab + b2)\n\nLet us find the zeros of the cubic polynomial function f(y) = y3 – 2y2 – y + 2.\n\n- Set f(y) = 0.  \n    y3 – 2y2 – y + 2 = 0.\n- Solve it.  \n    y2(y – 2) – (y – 2) = 0  \n    (y2 – 1) (y – 2) = 0  \n    (y + 1) (y – 1) (y– 2) = 0  \n    y = 1, -1 and 2.\n\nHence the zeros of the polynomial function are 1, -1, and 2.","x":-2060,"y":280,"width":250,"height":461},
		{"id":"638b6db461e2be6d","type":"text","text":"**Important Notes on Polynomial Functions:**\n---\nHere is a list of a few points that should be remembered while studying polynomial functions:\n\n- The degree of the polynomial function is determined by the highest power of the variable it is raised to.\n- [Constant](https://www.cuemath.com/algebra/constants/) functions are polynomial functions of degree 0.\n- Linear Functions are polynomial functions of degree 1.\n- Quadratic Functions are polynomial functions of degree 2.\n- Cubic Functions are polynomial functions of degree 3.","x":-1399,"y":1080,"width":250,"height":338},
		{"id":"2eacb0d74b0eb834","type":"text","text":"## Comparison of topologies\n---\n== Comparison of topologies ==\n---\n{{main|Comparison of topologies}}\n{{citations needed|section|date=May 2025}}\nMany topologies can be defined on a set to form a topological space. When every open set of a topology <math>\\tau_1</math> is also open for a topology <math>\\tau_2,</math> one says that <math>\\tau_2</math> is {{em|finer}} than <math>\\tau_1,</math> and <math>\\tau_1</math> is {{em|coarser}} than <math>\\tau_2.</math> A proof that relies only on the existence of certain open sets will also hold for any finer topology, and similarly a proof that relies only on certain sets not being open applies to any coarser topology.  The terms {{em|larger}} and {{em|smaller}} are sometimes used in place of finer and coarser, respectively.  The terms {{em|stronger}} and {{em|weaker}} are also used in the literature, but with little agreement on the meaning, so one should always be sure of an author's convention when reading.\n\nThe collection of all topologies on a given fixed set <math>X</math> forms a [[complete lattice]]: if <math>F = \\left\\{ \\tau_{\\alpha} : \\alpha \\in A \\right\\}</math> is a collection of topologies on <math>X,</math> then the [[Join and meet|meet]] of <math>F</math> is the intersection of <math>F,</math> and the [[Join and meet|join]] of <math>F</math> is the meet of the collection of all topologies on <math>X</math> that contain every member of <math>F.</math>","x":-411,"y":-923,"width":392,"height":403},
		{"id":"a7c03fcaa1816973","type":"text","text":"## Continuous functions\n---\nA [[Function (mathematics)|function]] <math>f : X \\to Y</math> between topological spaces is called '''[[Continuity (topology)|continuous]]''' if for every <math> x \\in X</math> and every neighbourhood <math>N</math> of <math>f(x)</math> there is a neighbourhood <math>M</math> of <math>x</math> such that <math>f(M) \\subseteq N.</math> This relates easily to the usual definition in analysis. Equivalently, <math>f</math> is continuous if the [[inverse image]] of every open set is open.{{sfn|Armstrong|1983|loc=theorem 2.6}}  This is an attempt to capture the intuition that there are no \"jumps\" or \"separations\" in the function.  A [[homeomorphism]] is a [[bijection]] that is continuous and whose [[Inverse function|inverse]] is also continuous.  Two spaces are called {{em|homeomorphic}} if there exists a homeomorphism between them.  From the standpoint of topology, homeomorphic spaces are essentially identical.<ref>{{Cite book|isbn = 978-93-325-4953-1|last = Munkres|first = James R|title = Topology|date = 2015|pages = 317–319| publisher=Pearson }}</ref>","x":-751,"y":-881,"width":250,"height":361},
		{"id":"e149326386b83367","type":"text","text":"category of topological spaces\n---\nIn [[category theory]], one of the fundamental [[Category (mathematics)|categories]] is '''Top''', which denotes the [[category of topological spaces]] whose [[Object (category theory)|objects]] are topological spaces and whose [[morphism]]s are continuous functions.  The attempt to classify the objects of this category ([[up to]] [[homeomorphism]]) by [[Invariant (mathematics)|invariant]]s has motivated areas of research, such as [[Homotopy|homotopy theory]], [[Homology (mathematics)|homology theory]], and [[K-theory]].{{cn|date=May 2025}}","x":-1054,"y":-934,"width":250,"height":414},
		{"id":"7237893a2c5b07e7","type":"text","text":"  \nList of topologies\n---\n{{short description|List of concrete topologies and topological spaces}}\n{{Main|List of topology topics}}\n\nThe following is a list of named [[Topology (structure)|topologies]] or [[topological space]]s, many of which are counterexamples in [[topology]] and related branches of [[mathematics]]. This is not a list of [[Topological property|properties]] that a topology or topological space might possess; for that, see [[List of general topology topics]] and [[Topological property]]. \n\n==Discrete and indiscrete==\n\n* [[Discrete topology]] − All subsets are open.\n* [[Indiscrete topology]], chaotic topology, or [[Trivial topology]] − Only the empty set and its complement are open.\n\n==Cardinality and ordinals==\n\n{{See also|Cardinality|Ordinal number}}\n\n* [[Cocountable topology]]\n** Given a topological space <math>(X, \\tau),</math> the ''[[Cocountable topology#Cocountable extension topology|{{dfn|cocountable extension topology}}]]'' on <math>X</math> is the topology having as a subbasis the union of {{math|τ}} and the family of all subsets of <math>X</math> whose complements in <math>X</math> are countable.\n* [[Cofinite topology]]\n* [[Double-pointed cofinite topology]]\n* [[Ordinal number topology]]\n* [[Pseudo-arc]]\n* [[Ran space]]\n* [[Tychonoff plank]]\n\n===Finite spaces===\n\n* [[Discrete two-point space]] − The simplest example of a [[totally disconnected]] [[discrete space]].\n* [[Finite topological space]]\n* [[Pseudocircle]] − A [[finite topological space]] on 4 elements that fails to satisfy any [[separation axiom]] besides [[Kolmogorov space|T<sub>0</sub>]]. However, from the viewpoint of [[algebraic topology]], it has the remarkable property that it is indistinguishable from the [[circle]] <math>S^1.</math>\n* [[Sierpiński space]], also called the [[connected two-point set]] − A 2-point set <math>\\{0, 1\\}</math> with the [[particular point topology]] <math>\\{\\varnothing, \\{1\\}, \\{0,1\\}\\}.</math>\n\n==Integers==\n\n* [[Arens–Fort space]] − A Hausdorff, regular, normal space that is not first-countable or compact. It has an element (i.e. <math>p := (0, 0)</math>) for which there is no sequence in <math>X \\setminus \\{p\\}</math> that converges to <math>p</math> but there is a sequence <math>x_\\bull = \\left(x_i\\right)_{i=1}^\\infty</math> in <math>X \\setminus \\{(0, 0)\\}</math> such that <math>(0, 0)</math> is a cluster point of <math>x_\\bull.</math>\n* [[Arithmetic progression topologies]]\n* [[Baire space (set theory)|The Baire space]] − <math>\\N^{\\N}</math> with the product topology, where <math>\\N</math> denotes the [[natural numbers]] endowed with the discrete topology. It is the space of all sequences of natural numbers.\n* [[Divisor topology]]\n* [[Partition topology]]\n** [[Deleted integer topology]]\n** [[Odd–even topology]]\n\n==Fractals and Cantor set==\n\n{{See also|List of fractals by Hausdorff dimension|Fractal}}\n\n* [[Apollonian gasket]]\n* [[Cantor set]] − A subset of the closed interval <math>[0, 1]</math> with remarkable properties.\n** [[Cantor dust]]\n** [[Cantor space]]\n* [[Koch snowflake]]\n* [[Menger sponge]]\n* [[Mosely snowflake]]\n* [[Sierpiński carpet]]\n* [[Sierpiński triangle]]\n* [[Smith–Volterra–Cantor set]], also called the {{em|fat Cantor set}} − A closed [[nowhere dense]] (and thus [[Meagre set|meagre]]) subset of the unit interval <math>[0, 1]</math> that has positive [[Lebesgue measure]] and is not a [[Jordan measurable set]]. The complement of the fat Cantor set in Jordan measure is a bounded open set that is not Jordan measurable.\n\n==Orders==\n\n{{See also|Preorder|Partially ordered set}}\n\n* [[Alexandrov topology]]\n* [[Lexicographic order topology on the unit square]]\n* [[Order topology]]\n** [[Lawson topology]]\n** [[Poset topology]]\n** [[Upper topology]]\n** [[Scott topology]]\n*** [[Scott continuity]]\n* [[Priestley space]]\n* [[Roy's lattice space]]\n* [[Split interval]], also called the ''{{dfn|Alexandrov double arrow space}}'' and the ''{{dfn|two arrows space}}'' − All compact separable ordered spaces are order-isomorphic to a subset of the split interval. It is [[Compact Hausdorff space|compact Hausdorff]], [[Hereditarily Lindelöf space|hereditarily Lindelöf]], and [[Hereditarily separable space|hereditarily separable]] but not [[metrizable space|metrizable]]. Its metrizable [[Subspace topology|subspace]]s are all countable.\n* [[Specialization (pre)order]]\n\n==Manifolds and complexes==\n\n{{See also|Topological manifold|Smooth manifold}}\n\n* [[Branching line]] − A [[non-Hausdorff manifold]].\n* [[Double origin topology]]\n* [[E8 manifold|E<sub>8</sub> manifold]] − A [[topological manifold]] that does not admit a [[smooth structure]].\n* [[Euclidean topology]] − The [[natural topology]] on [[Euclidean space]] <math>\\Reals^n</math> induced by the [[Euclidean distance|Euclidean metric]], which is itself induced by the [[Euclidean norm]]. \n** [[Real line]] − <math>\\Reals</math>\n** [[Unit interval]] − <math>[0, 1]</math>\n* [[Extended real number line]]\n* [[Fake 4-ball]] − A compact [[contractible]] topological [[4-manifold]].\n* [[House with two rooms]] − A [[contractible]], 2-dimensional  [[simplicial complex]] that is not [[Collapse (topology)|collapsible]].\n* [[Klein bottle]]\n* [[Lens space]]\n* [[Line with two origins]], also called the ''{{dfn|bug-eyed line}}'' − It is a [[non-Hausdorff manifold]]. It is [[locally homeomorphic]] to [[Euclidean space]] and thus [[locally metrizable space|locally metrizable]] (but not [[Metrizable space|metrizable]]) and [[Locally Hausdorff space|locally Hausdorff]] (but not [[Hausdorff space|Hausdorff]]). It is also a [[T1 space|T<sub>1</sub>]] [[locally regular space]] but not a [[semiregular space]].\n* [[Prüfer manifold]] − A Hausdorff 2-dimensional real analytic manifold that is not [[Paracompact space|paracompact]].\n* [[Real projective line]]\n* [[Torus]]\n** [[3-torus]]\n** [[Solid torus]]\n* [[Unknot]]\n* [[Whitehead manifold]] − An open [[3-manifold]] that is [[contractible]], but not [[homeomorphic]] to <math>\\Reals^3.</math>\n\n===Hyperbolic geometry===\n\n* [[Gieseking manifold]] − A cusped hyperbolic 3-manifold of finite volume.\n* [[Horosphere]]\n** [[Horocycle]]\n* [[Picard horn]]\n* [[Seifert–Weber space]]\n\n===Paradoxical spaces===\n\n* [[Lakes of Wada]] − Three disjoint connected open sets of <math>\\Reals^2</math> or <math>(0, 1)^2</math> that all have the same boundary.\n\n===Unique===\n\n* [[Hantzsche–Wendt manifold]] − A compact, orientable, [[Flat manifold|flat]] 3-manifold. It is the only closed flat 3-manifold with first [[Betti number]] zero.\n\n===Related or similar to manifolds===\n\n* [[Dogbone space]]\n* [[Dunce hat (topology)]]\n* [[Hawaiian earring]]\n* [[Long line (topology)]]\n* [[Rose (topology)]]\n\n==Embeddings and maps between spaces==\n\n* [[Alexander horned sphere]] − A particular embedding of a [[sphere]] into 3-dimensional Euclidean space.\n* [[Antoine's necklace]] − A topological embedding of the [[Cantor set]] in 3-dimensional Euclidean space, whose complement is not [[simply connected]].\n* [[Irrational winding of a torus]]/[[Irrational cable on a torus]]\n* [[Knot (mathematics)]]\n* [[Linear flow on the torus]]\n* [[Space-filling curve]]\n* [[Torus knot]]\n* [[Wild knot]]\n\n==Counter-examples (general topology)==\n\nThe following topologies are a known source of counterexamples for [[Topology|point-set topology]]. \n\n* [[Alexandroff plank]]\n* [[Appert topology]] − A Hausdorff, [[perfectly normal space|perfectly normal]] (T<sub>6</sub>), [[zero-dimensional space]] that is countable, but neither [[first countable]], [[Locally compact space|locally compact]], nor [[countably compact]].\n* [[Arens square]]\n* [[Bullet-riddled square]] - The space <math>[0, 1]^2 \\setminus \\Q^2,</math> where <math>[0, 1]^2 \\cap \\Q^2</math> is the set of bullets. Neither of these sets is [[Jordan measure|Jordan measurable]] although both are [[Lebesgue measurable]]. \n* [[Cantor tree]]\n* [[Comb space]]\n* [[Dieudonné plank]]\n* [[Double origin topology]]\n* [[Dunce hat (topology)]]\n* [[Either–or topology]]\n* [[Excluded point topology]] − A topological space where the open sets are defined in terms of the exclusion of a particular point.\n* [[Fort space]]\n* [[Half-disk topology]]\n* [[Hilbert cube]] − <math>[0, 1/1] \\times [0, 1/2] \\times [0, 1/3] \\times \\cdots</math> with the [[product topology]].\n* [[Infinite broom]]\n* [[Integer broom topology]]\n* [[K-topology]]\n* [[Knaster–Kuratowski fan]]\n* [[Long line (topology)]]\n* [[Moore plane]], also called the ''{{dfn|Niemytzki plane}}'' − A [[first countable]], [[Separable space|separable]], [[completely regular]], Hausdorff, [[Moore space (topology)|Moore space]] that is not [[normal space|normal]], [[Lindelöf space|Lindelöf]], [[metrizable]], [[second countable]], nor [[locally compact]]. It also an uncountable closed subspace with the discrete topology.\n* [[Nested interval topology]]\n* [[Overlapping interval topology]] − Second countable space that is T<sub>0</sub> but not T<sub>1</sub>.\n* [[Particular point topology]] − Assuming the set is infinite, then contains a non-closed compact subset whose closure is not compact and moreover, it is neither [[Metacompact space|metacompact]] nor [[Paracompact space|paracompact]].\n* [[Rational sequence topology]]\n* [[Sorgenfrey line]], which is <math>\\Reals</math> endowed with [[lower limit topology]] − It is Hausdorff, perfectly normal, first-countable, separable, paracompact, Lindelöf, [[Baire space|Baire]], and a [[Moore space (topology)|Moore space]] but not metrizable, second-countable, σ-compact, nor locally compact.\n* [[Sorgenfrey plane]], which is the product of two copies of the Sorgenfrey line − A [[Moore space (topology)|Moore space]] that is neither [[normal space|normal]], [[Paracompact space|paracompact]], nor [[second countable]].\n* [[Topologist's sine curve]]\n* [[Tychonoff plank]]\n* [[Vague topology]]\n* [[Warsaw circle]]\n\n==Topologies defined in terms of other topologies==\n\n===Natural topologies===\n\nList of [[Natural topology|natural topologies]]. \n\n* [[Adjunction space]]\n* [[Disjoint union (topology)]]\n* [[Extension topology]]\n* [[Initial topology]]\n* [[Final topology]]\n* [[Product topology]]\n* [[Quotient topology]]\n* [[Subspace topology]]\n* [[Weak topology]]\n\n===Compactifications===\n\n[[Compactification (mathematics)|Compactifications]] include:\n\n* [[Alexandroff extension]]\n** [[Projectively extended real line]]\n* [[Bohr compactification]]\n* [[Eells–Kuiper manifold]]\n* [[Projectively extended real line]]\n* [[Stone–Čech compactification]]\n** [[Stone topology]]\n** [[Stone–Čech remainder]]\n* [[Wallman compactification]]\n\n===Topologies of uniform convergence===\n\nThis lists named topologies of [[uniform convergence]].\n\n* [[Compact-open topology]]\n** [[Loop space]]\n* [[Interlocking interval topology]]\n* [[Modes of convergence]] ([[Modes of convergence (annotated index)|annotated index]])\n* [[Operator topologies]]\n* [[Pointwise convergence]]\n** [[Weak convergence (Hilbert space)]]\n** [[Weak* topology]]\n* [[Polar topology]]\n* [[Strong dual space|Strong dual topology]]\n* [[Topologies on spaces of linear maps]]\n\n===Other induced topologies===\n\n* [[Box topology]]\n* [[Compact complement topology]]\n* <em>[[Duplication of a point]]</em>: Let <math>x \\in X</math> be a non-[[isolated point]] of <math>X,</math> let <math>d \\not\\in X</math> be arbitrary, and let <math>Y = X \\cup \\{d\\}.</math> Then <math>\\tau = \\{V \\subseteq Y : \\text{ either } V \\text{ or } ( V \\setminus \\{d\\}) \\cup \\{x\\} \\text{ is an open subset of } X\\}</math> is a topology on <math>Y</math> and <math>x</math> and <math>d</math> have the same [[neighborhood filter]]s in <math>Y.</math> In this way, <math>x</math> has been duplicated.{{sfn|Wilansky|2008|p=35}}\n* [[Extension topology]]\n\n==Functional analysis==\n\n* [[Auxiliary normed spaces]]\n* [[Finest locally convex topology]]\n* [[Finest vector topology]]\n* [[Helly space]]\n* [[Mackey topology]]\n* [[Polar topology]]\n* [[Vague topology]]\n\n===Operator topologies===\n\n* [[Dual topology]]\n* [[Norm topology]]\n* [[Operator topologies]]\n* [[Pointwise convergence]]\n** [[Weak convergence (Hilbert space)]]\n** [[Weak* topology]]\n* [[Polar topology]]\n* [[Strong dual space]]\n* [[Strong operator topology]]\n* [[Topologies on spaces of linear maps]]\n* [[Ultrastrong topology]]\n* [[Ultraweak topology]]/[[weak-* operator topology]]\n* [[Weak operator topology]]\n\n===Tensor products===\n\n* [[Inductive tensor product]]\n* [[Injective tensor product]]\n* [[Projective tensor product]]\n* [[Tensor product of Hilbert spaces]]\n* [[Topological tensor product]]\n\n== Probability ==\n* [[Émery topology]]\n\n==Other topologies==\n\n* [[Erdős space]] − A Hausdorff, [[totally disconnected]], [[Dimension theory|one-dimensional]] topological space <math>X</math> that is homeomorphic to <math>X \\times X.</math>\n* [[Half-disk topology]]\n* [[Hedgehog space]]\n* [[Partition topology]]\n* [[Zariski topology]]\n\n==See also==\n\n* {{annotated link|Counterexamples in Topology|''Counterexamples in Topology''}}\n* {{annotated link|List of Banach spaces}}\n* {{annotated link|List of fractals by Hausdorff dimension}}\n* {{annotated link|List of manifolds}}\n* {{annotated link|List of topologies on the category of schemes}}\n* {{annotated link|List of topology topics}}\n* {{annotated link|Lists of mathematics topics}}\n* {{annotated link|Natural topology}}\n* {{annotated link|Table of Lie groups}}\n\n==Citations==\n\n{{reflist}}\n{{reflist|group=note}}\n\n==References==\n\n{{refbegin|2}}\n* {{Adams Franzosa Introduction to Topology Pure and Applied}} <!--{{sfn|Adams|Franzosa|2009|p=}}-->\n* {{Arkhangel'skii Ponomarev Fundamentals of General Topology Problems and Exercises|edition=2}} <!--{{sfn|Arkhangelʹskiĭ|Ponomarev|1984|p=}}-->\n* {{Bourbaki General Topology Part I Chapters 1-4}} <!--{{sfn|Bourbaki|1989|p=}}-->\n* {{Bourbaki General Topology Part II Chapters 5-10}} <!--{{sfn|Bourbaki|1989|p=}}-->\n* {{Comfort Negrepontis The Theory of Ultrafilters 1974}} <!--{{sfn|Comfort|Negrepontis|1974|p=}}-->\n* {{Dixmier General Topology}} <!--{{sfn|Dixmier|1984|p=}}-->\n* {{Császár General Topology}} <!--{{sfn|Császár|1978|p=}}-->\n* {{Dolecki Mynard Convergence Foundations Of Topology}} <!--{{sfn|Dolecki|Mynard|2016|p=}}-->\n* {{Dugundji Topology}} <!--{{sfn|Dugundji|1966|p=}}-->\n* {{Howes Modern Analysis and Topology 1995}} <!--{{sfn|Howes|1995|p=}}-->\n* {{Jarchow Locally Convex Spaces}} <!--{{sfn|Jarchow|1981|p=}}-->\n* {{Joshi Introduction to General Topology}} <!--{{sfn|Joshi|1983|p=}}-->\n* {{Kelley General Topology}} <!--{{sfn|Kelley|1975|p=}}-->\n* {{Köthe Topological Vector Spaces I}} <!--{{sfn|Köthe|1969|p=}}-->\n* {{Munkres Topology|edition=2}} <!--{{sfn|Munkres|2000|p=}}-->\n* {{Schechter Handbook of Analysis and Its Foundations}} <!--{{sfn|Schechter|1996|p=}}-->\n* {{Schubert Topology}} <!--{{sfn|Schubert|1968|p=}}-->\n* {{Wilansky Modern Methods in Topological Vector Spaces|edition=1}} <!--{{sfn|Wilansky|2013|p=}}-->\n* {{Wilansky Topology for Analysis 2008}} <!--{{sfn|Wilansky|2008|p=}}-->\n* {{Willard General Topology}} <!--{{sfn|Willard|2004|p=}}-->\n{{refend}}\n\n==External links==\n\n* [http://topology.jdabbs.com π-Base: An Interactive Encyclopedia of Topological Spaces]\n\n{{Topology}}\n\n{{DEFAULTSORT:Topologies}}\n\n[[Category:General topology]]\n[[Category:Mathematics-related lists]]\n[[Category:Topological spaces]]\n[[Category:Topology| ]]\n","x":-1054,"y":-1412,"width":1114,"height":412},
		{"id":"28dc528c201c17b4","type":"text","text":"# List of general topology topics\n---\n{{Short description|none}}\nThis is a '''list of [[general topology]] topics'''.\n\n==Basic concepts==\n\n*[[Topological space]]\n*[[Topological property]]\n*[[Open set]], [[closed set]]\n**[[Clopen set]]\n**[[Closure (topology)|Closure]]\n**[[Boundary (topology)|Boundary]]\n**[[Dense (topology)|Density]]\n**[[G-delta set]], [[F-sigma set]]\n**[[Closeness (mathematics)|Closeness]]\n**[[Neighbourhood (mathematics)|Neighborhood]]\n*[[Continuity (topology)]]\n**[[Homeomorphism]]\n**[[Local homeomorphism]]\n**[[Open and closed maps]]\n**[[Embedding#General topology|Embedding]]\n**[[Germ (mathematics)|Germ]]\n*[[Base (topology)|Basis]]\n**[[Subbase|Subbasis]]\n*[[Open cover]]\n*[[Locally finite space]]\n*[[Covering space]]\n*[[Atlas (topology)|Atlas]]\n\n==Limits==\n\n*[[Limit point]]\n*[[Net (topology)|Net]]\n*[[Filter (topology)|Filter]]\n*[[Ultrafilter]]\n\n==[[Topological properties]]==\n\n*[[Baire category theorem]]\n**[[Nowhere dense]]\n**[[Baire space]]\n**[[Banach–Mazur game]]\n**[[Meagre set]]\n**[[Comeagre set]]\n\n===Compactness and countability===\n\n*[[Compact space]]\n**[[Relatively compact subspace]]\n**[[Heine–Borel theorem]]\n**[[Tychonoff's theorem]]\n**[[Finite intersection property]]\n**[[Compactification (mathematics)|Compactification]]\n**[[Measure of non-compactness]]\n*[[Paracompact space]]\n*[[Locally compact space]]\n*[[Compactly generated space]]\n*[[Axiom of countability]]\n*[[Sequential space]]\n*[[First-countable space]]\n*[[Second-countable space]]\n*[[Separable space]]\n*[[Lindelöf space]]\n*[[Sigma-compact space]]\n\n===Connectedness===\n\n*[[Connected space]]\n*[[Simply connected space]]\n*[[Path connected space]]\n\n===[[Separation axioms]]===\n\n*[[T0 space|T<sub>0</sub> space]]\n*[[T1 space|T<sub>1</sub> space]]\n*[[Hausdorff space]]\n**[[Completely Hausdorff space]]\n*[[Regular space]]\n*[[Tychonoff space]]\n*[[Normal space]]\n*[[Urysohn's lemma]]\n*[[Tietze extension theorem]]\n*[[Paracompact]]\n*[[Separated sets]]\n\n==Topological constructions==\n\n*[[direct sum (topology)|Direct sum]] and the dual construction [[product (topology)|product]]\n*[[subspace (topology)|Subspace]] and the dual construction [[quotient (topology)|quotient]]\n*[[Topological tensor product]]\n\n==Examples==\n\n{{See also|List of examples in general topology}}\n\n*[[Discrete space]]\n**[[Locally constant function]]\n*[[Trivial topology]]\n*[[Cofinite topology]]\n*[[Cocountable topology]]\n*[[Finer topology]]\n*[[Product topology]]\n**[[Restricted product]]\n*[[Quotient space (topology)|Quotient space]]\n*[[Unit interval]]\n*[[Continuum (topology)|Continuum]]\n*[[Extended real number line]]\n*[[Long line (topology)]]\n*[[Sierpinski space]]\n*[[Cantor set]], [[Cantor space]], [[Cantor cube]]\n*[[Space-filling curve]]\n*[[Topologist's sine curve]]\n*[[Tychonoff plank]]\n*[[Comb space]]\n*[[Uniform norm]]\n*[[Weak topology]]\n*[[Strong topology (polar topology)|Strong topology]]\n*[[Hilbert cube]]\n*[[Lower limit topology]]\n*[[Sorgenfrey plane]]\n*[[Real tree]]\n*[[Compact-open topology]]\n*[[Zariski topology]]\n*[[Kuratowski closure axioms]]\n*[[Unicoherent]]\n*[[Solenoid (mathematics)]]\n\n==[[Uniform space]]s==\n\n*[[Uniform continuity]]\n*[[Lipschitz continuity]]\n*[[Uniform isomorphism]]\n*[[Uniform property]]\n*[[Uniformly connected space]]\n\n==[[Metric space]]s==\n\n*[[Metric topology]]\n*[[Manhattan distance]]\n*[[Ultrametric space]]\n**[[P-adic numbers]], [[p-adic analysis]]\n*[[Open ball]]\n*[[Bounded subset]]\n*[[Pointwise convergence]]\n*[[Metrization theorems]]\n*[[Complete space]]\n**[[Cauchy sequence]]\n**[[Banach fixed-point theorem]]\n*[[Polish space]]\n*[[Hausdorff distance]]\n*[[Intrinsic metric]]\n*[[Category of metric spaces]]\n\n==Topology and [[order theory]]==\n*[[Stone duality]]\n**[[Stone's representation theorem for Boolean algebras]]\n*[[Specialization (pre)order]]\n*[[Sober space]]\n*[[Spectral space]]\n*[[Alexandrov topology]]\n*[[Upper topology]]\n*[[Scott topology]]\n**[[Scott continuity]]\n*[[Lawson topology]]\n\n==[[Descriptive set theory]]==\n\n* [[Polish space|Polish Space]]\n* [[Cantor space]]\n\n==[[Dimension theory]]==\n{{main article|Dimension}}\n\n*[[Inductive dimension]]\n*[[Lebesgue covering dimension]]\n*[[Lebesgue's number lemma]]\n\n==[[Combinatorial topology]]==\n\n*[[Polytope]]\n*[[Simplex]]\n*[[Simplicial complex]]\n*[[CW complex]]\n*[[Manifold]]\n*[[Triangulation]]\n*[[Barycentric subdivision]]\n*[[Sperner's lemma]]\n*[[Simplicial approximation theorem]]\n*[[Nerve of an open covering]]\n\n==Foundations of [[algebraic topology]]==\n\n*[[Simply connected]]\n*[[Semi-locally simply connected]]\n*[[Path (topology)]]\n*[[Homotopy]]\n*[[Homotopy lifting property]]\n*[[Pointed space]]\n*[[Wedge sum]]\n*[[Smash product]]\n*[[Cone (topology)]]\n*[[Adjunction space]]\n\n==Topology and algebra==\n\n*[[Topological algebra]]\n*[[Topological group]]\n*[[Topological ring]]\n*[[Topological vector space]]\n*[[Topological module]]\n*[[Topological abelian group]]\n*[[Properly discontinuous]]\n*[[Sheaf space]]\n\n==See also==\n*[[Topology glossary]]\n*[[List of topologies]]\n*[[List of topology topics]]\n*[[List of geometric topology topics]]\n*[[List of algebraic topology topics]]\n*[[List of publications in mathematics#Topology|Publications in topology]]\n\n\n{{Topology |expanded}}\n{{Outlines |collapsed}}\n\n{{DEFAULTSORT:Topology Topics, General}}\n[[Category:General topology|*]]\n[[Category:Mathematics-related lists]]\n[[Category:Outlines of mathematics and logic]]\n[[Category:Outlines]]\n","x":-1054,"y":-1795,"width":1114,"height":435},
		{"id":"4dc62aa3f4b751a9","type":"text","text":"graph dynamical system\n---\n==Formal definition==\n\nA graph dynamical system is constructed from the following components:\n\n<blockquote>\n* A finite ''graph'' ''Y'' with vertex set v[''Y''] = {1,2, ... , n}. Depending on the context the graph can be directed or undirected.\n* A state ''x<sub>v</sub>'' for each vertex ''v'' of ''Y'' taken from a finite set ''K''. The ''system state'' is the ''n''-tuple ''x'' = (''x''<sub>1</sub>, ''x''<sub>2</sub>, ... , ''x<sub>n</sub>''), and ''x''[''v''] is the tuple consisting of the states associated to the vertices in the 1-neighborhood of ''v'' in ''Y'' (in some fixed order).\n* A ''vertex function'' ''f<sub>v</sub>'' for each vertex ''v''. The vertex function maps the state of vertex ''v'' at time ''t'' to the vertex state at time ''t''&nbsp;+&nbsp;1 based on the states associated to the 1-neighborhood of ''v'' in ''Y''.\n* An ''update scheme'' specifying the mechanism by which the mapping of individual vertex states is carried out so as to induce a discrete dynamical system with map ''F'': ''K<sup>n</sup> → K<sup>n</sup>''.\n</blockquote>\n\nThe ''phase space'' associated to a dynamical system with map ''F'': ''K<sup>n</sup> → K<sup>n</sup>'' is the finite directed graph with vertex set ''K<sup>n</sup>'' and directed edges (''x'', ''F''(''x'')). The structure of the phase space is governed by the properties of the graph ''Y'', the vertex functions (''f<sub>i</sub>'')''<sub>i</sub>'', and the update scheme. The research in this area seeks to infer phase space properties based on the structure of the system constituents. The analysis has a local-to-global character.\n","x":-2053,"y":-351,"width":333,"height":391},
		{"id":"83ab061c45b01097","type":"text","text":"Extracting the dynamics of a single scalar variable from a first-order matrix system\n---\n==Extracting the dynamics of a single scalar variable from a first-order matrix system==\n\nStarting from the {{math|''n''}}-dimensional system {{math|'''y'''<sub>''t''</sub> {{=}} '''Ay'''<sub>''t''−1</sub>}}, we can extract the dynamics of one of the state variables, say {{math|''y''<sub>1</sub>}}. The above solution equation for {{mvar|y<sub>t</sub>}} shows that the solution for {{math|''y''<sub>1,''t''</sub>}} is in terms of the {{mvar|n}} eigenvalues of {{math|'''A'''}}.  Therefore the equation describing the evolution of {{math|''y''<sub>1</sub>}} by itself must have a solution involving those same eigenvalues.  This description intuitively motivates the equation of evolution of {{math|''y''<sub>1</sub>}}, which is\n\n:<math> y_{1,t} = a_1 y_{1,t-1} + a_2 y_{1,t-2} + \\dots + a_n y_{1,t-n}</math>\n\nwhere the parameters {{mvar|a<sub>i</sub>}} are from the [[Characteristic equation (calculus)|characteristic equation]] of the matrix {{math|'''A'''}}:\n\n:<math>\\lambda^{n} - a_1 \\lambda^{n-1} - a_2 \\lambda^{n-2} - \\dots - a_n \\lambda^{0} = 0.</math>\n\nThus each individual scalar variable of an {{mvar|n}}-dimensional first-order [[linear system]] evolves according to a univariate {{mvar|n}}th-degree difference equation, which has the same stability property (stable or unstable) as does the matrix difference equation.\n","x":-2127,"y":-1673,"width":250,"height":553},
		{"id":"1b97ba7ba89e00ef","type":"text","text":"Solution and stability of higher-order cases\n---\n==Solution and stability of higher-order cases==\n\nMatrix difference equations of higher order&mdash;that is, with a time lag longer than one period&mdash;can be solved, and their stability analyzed, by converting them into first-order form using a [[block matrix]] (matrix of matrices).  For example, suppose we have the second-order equation\n\n:<math>\\mathbf x_t = \\mathbf{Ax}_{t-1} + \\mathbf{Bx}_{t-2}</math>\n\nwith the variable vector {{math|'''x'''}} being {{math|''n'' × 1}} and {{math|'''A'''}} and {{math|'''B'''}} being {{math|''n'' × ''n''}}.  This can be stacked in the form\n\n:<math>\\begin{bmatrix}\\mathbf{x}_t \\\\ \\mathbf{x}_{t-1} \\\\ \\end{bmatrix} = \\begin{bmatrix} \\mathbf{A} & \\mathbf{B} \\\\ \\mathbf{I} & \\mathbf{0} \\\\ \\end{bmatrix} \\begin{bmatrix} \\mathbf{x}_{t-1} \\\\ \\mathbf{x}_{t-2} \\end{bmatrix} </math>\n\nwhere {{math|'''I'''}} is the {{math|''n'' × ''n''}} [[identity matrix]] and {{math|'''0'''}} is the {{math|''n'' × ''n''}} [[zero matrix]].  Then denoting the {{math|2''n'' × 1}} stacked vector of current and once-lagged variables as {{math|'''z'''<sub>''t''</sub>}} and the {{math|2''n'' × 2''n''}} block matrix as {{math|'''L'''}}, we have as before the solution \n\n:<math>\\mathbf z_t = \\mathbf L^t \\mathbf z_0 </math>\n\nAlso as before, this stacked equation, and thus the original second-order equation, are stable if and only if all eigenvalues of the matrix {{math|'''L'''}} are smaller than unity in absolute value.\n","x":-1762,"y":-1691,"width":250,"height":571},
		{"id":"890404c1cbb3df75","type":"text","text":"# Matrix difference equation\n---\n{{short description|Relation of a matrix of variables between two points in time}}\n{{further|Linear recurrence with constant coefficients}}\n\nA '''matrix difference equation''' is a [[difference equation]] in which the value of a [[Euclidean vector#Representations|vector]] (or sometimes, a matrix) of variables at one point in time is related to its own value at one or more previous points in time, using [[Matrix (mathematics)|matrices]].<ref>{{cite book|last1=Cull|first1=Paul|author2-link=Mary Flahive|last2=Flahive|first2=Mary|last3=Robson|first3=Robbie|title=Difference Equations: From Rabbits to Chaos|title-link= Difference Equations: From Rabbits to Chaos |publisher=Springer|date=2005|at=ch. 7|isbn=0-387-23234-6}}</ref><ref>{{cite book|last=Chiang|first=Alpha C.|title=Fundamental Methods of Mathematical Economics|url=https://archive.org/details/fundamentalmetho0000chia_h4v2|url-access=registration|edition=3rd|publisher=McGraw-Hill|date=1984|pages=[https://archive.org/details/fundamentalmetho0000chia_h4v2/page/608 608–612]|isbn=9780070107809 }}</ref> The '''order''' of the equation is the maximum time gap between any two indicated values of the variable vector.  For example,\n\n:<math>\\mathbf x_t = \\mathbf{Ax}_{t-1} + \\mathbf{Bx}_{t-2}</math>\n\nis an example of a second-order matrix difference equation, in which {{math|'''x'''}} is an {{math|''n'' × 1}} vector of variables and {{math|'''A'''}} and {{math|'''B'''}} are {{math|''n'' × ''n''}} matrices.  This equation is homogeneous because there is no vector [[constant term]] added to the end of the equation.  The same equation might also be written as \n\n:<math>\\mathbf x_{t+2} = \\mathbf{Ax}_{t+1} + \\mathbf{Bx}_{t}</math>\n\nor as \n\n:<math>\\mathbf x_n = \\mathbf{Ax}_{n-1} + \\mathbf{Bx}_{n-2}</math>\n\nThe most commonly encountered matrix difference equations are first-order.\n","x":-2053,"y":-2080,"width":573,"height":218},
		{"id":"d4f4182966ff4610","type":"text","text":"Find Block Design and Soverign Identies in logs\n---\nIn [[mathematics]], a '''recurrence relation''' is an [[equation]] according to which the <math>n</math>th term of a [[sequence]] of numbers is equal to some combination of the previous terms. Often, only <math>k</math> previous terms of the sequence appear in the equation, for a parameter <math>k</math> that is independent of <math>n</math>; this number <math>k</math> is called the ''order'' of the relation. If the values of the first <math>k</math> numbers in the sequence have been given, the rest of the sequence can be calculated by repeatedly applying the equation.\n\nIn ''linear recurrences'', the {{mvar|n}}th term is equated to a [[linear function]] of the <math>k</math> previous terms. A famous example is the recurrence for the [[Fibonacci number]]s,\n<math display=block>F_n=F_{n-1}+F_{n-2}</math>\nwhere the order <math>k</math> is two and the linear function merely adds the two previous terms. This example is a [[linear recurrence with constant coefficients]], because the coefficients of the linear function (1 and 1) are constants that do not depend on <math>n.</math> For these recurrences, one can express the general term of the sequence as a [[closed-form expression]] of <math>n</math>. As well, [[P-recursive equation|linear recurrences with polynomial coefficients]] depending on <math>n</math> are also important, because many common [[elementary functions]] and [[special functions]] have a [[Taylor series]] whose coefficients satisfy such a recurrence relation (see [[holonomic function]]).\n\nSolving a recurrence relation means obtaining a [[closed-form solution]]: a non-recursive function of <math>n</math>.\n\nThe concept of a recurrence relation can be extended to [[multidimensional array]]s, that is, [[indexed families]] that are indexed by [[tuple]]s of [[natural number]]s.","x":-2360,"y":-2080,"width":250,"height":407},
		{"id":"b2c0dcdfb7b39db9","type":"text","text":"# Recurrence relation\n---\nIn [[mathematics]], a '''recurrence relation''' is an [[equation]] according to which the <math>n</math>th term of a [[sequence]] of numbers is equal to some combination of the previous terms. Often, only <math>k</math> previous terms of the sequence appear in the equation, for a parameter <math>k</math> that is independent of <math>n</math>; this number <math>k</math> is called the ''order'' of the relation. If the values of the first <math>k</math> numbers in the sequence have been given, the rest of the sequence can be calculated by repeatedly applying the equation.\n\nIn ''linear recurrences'', the {{mvar|n}}th term is equated to a [[linear function]] of the <math>k</math> previous terms. A famous example is the recurrence for the [[Fibonacci number]]s,\n<math display=block>F_n=F_{n-1}+F_{n-2}</math>\nwhere the order <math>k</math> is two and the linear function merely adds the two previous terms. This example is a [[linear recurrence with constant coefficients]], because the coefficients of the linear function (1 and 1) are constants that do not depend on <math>n.</math> For these recurrences, one can express the general term of the sequence as a [[closed-form expression]] of <math>n</math>. As well, [[P-recursive equation|linear recurrences with polynomial coefficients]] depending on <math>n</math> are also important, because many common [[elementary functions]] and [[special functions]] have a [[Taylor series]] whose coefficients satisfy such a recurrence relation (see [[holonomic function]]).\n\nSolving a recurrence relation means obtaining a [[closed-form solution]]: a non-recursive function of <math>n</math>.\n\nThe concept of a recurrence relation can be extended to [[multidimensional array]]s, that is, [[indexed families]] that are indexed by [[tuple]]s of [[natural number]]s.\n\n==Definition==\nA ''recurrence relation'' is an equation that expresses each element of a [[sequence]] as a function of the preceding ones. More precisely, in the case where only the immediately preceding element is involved, a recurrence relation has the form\n:<math>u_n=\\varphi(n, u_{n-1})\\quad\\text{for}\\quad n>0,</math>\nwhere \n:<math>\\varphi:\\mathbb N\\times X \\to X</math>\nis a function, where {{mvar|X}} is a set to which the elements of a sequence must belong. For any <math>u_0\\in X</math>, this defines a unique sequence with <math>u_0</math> as its first element, called the ''initial value''.<ref>Jacobson, Nathan, Basic Algebra 2 (2nd ed.), § 0.4. pg 16.</ref>\n\nIt is easy to modify the definition for getting sequences starting from the term of index 1 or higher.\n\nThis defines recurrence relation of ''first order''. A recurrence relation of ''order'' {{mvar|k}} has the form \n:<math>u_n=\\varphi(n, u_{n-1}, u_{n-2}, \\ldots, u_{n-k})\\quad\\text{for}\\quad n\\ge k,</math>\n\nwhere <math>\\varphi: \\mathbb N\\times X^k \\to X</math> is a function that involves {{mvar|k}} consecutive elements of the sequence.\nIn this case, {{mvar|k}} initial values are needed for defining a sequence.\n\n==Examples==\n===Factorial===\nThe [[factorial]] is defined by the recurrence relation\n:<math>n!=n\\cdot (n-1)!\\quad\\text{for}\\quad n>0,</math>\nand the initial condition\n:<math>0!=1.</math>\nThis is an example of a ''linear recurrence with polynomial coefficients'' of order 1, with the simple polynomial (in {{mvar|n}})\n:<math>n</math>\nas its only coefficient.\n\n===Logistic map===\nAn example of a recurrence relation is the [[logistic map]] defined by\n\n:<math>x_{n+1} = r x_n (1 - x_n),</math>\n\nfor a given constant <math>r.</math> The behavior of the sequence depends dramatically on <math>r,</math> but is stable when the initial condition <math>x_0</math> varies.\n\n===Fibonacci numbers===\nThe recurrence of order two satisfied by the [[Fibonacci number]]s is the canonical example of a homogeneous [[linear recurrence]] relation with constant coefficients (see below).  The Fibonacci sequence is defined using the recurrence\n\n:<math>F_n = F_{n-1}+F_{n-2}</math>\n\nwith [[initial condition]]s\n\n:<math>F_0 = 0</math>\n:<math>F_1 = 1.</math>\n\nExplicitly, the recurrence yields the equations\n:<math>F_2 = F_1 + F_0</math>\n:<math>F_3 = F_2 + F_1</math>\n:<math>F_4 = F_3 + F_2</math>\netc.\n\nWe obtain the sequence of Fibonacci numbers, which begins\n:0, 1, 1, 2, 3, 5, 8, 13, 21, 34, 55, 89, ...\n\nThe recurrence can be solved by methods described below yielding [[Binet's formula]], which involves powers of the two roots of the characteristic polynomial <math>t^2 = t + 1</math>; the [[generating function]] of the sequence is the [[rational function]]\n:  <math>\\frac{t}{1-t-t^2}.</math>\n\n===Binomial coefficients===\nA simple example of a multidimensional recurrence relation is given by the [[binomial coefficient]]s <math>\\tbinom{n}{k}</math>, which count the ways of selecting <math>k</math> elements out of a set of <math>n</math> elements.\nThey can be computed by the recurrence relation\n:<math>\\binom{n}{k}=\\binom{n-1}{k-1}+\\binom{n-1}{k},</math>\nwith the base cases <math>\\tbinom{n}{0}=\\tbinom{n}{n}=1</math>. Using this formula to compute the values of all binomial coefficients generates an infinite array called [[Pascal's triangle]]. The same values can also be computed directly by a different formula that is not a recurrence, but uses [[factorial]]s, multiplication and division, not just additions:\n:<math>\\binom{n}{k}=\\frac{n!}{k!(n-k)!}.</math>\n\nThe binomial coefficients can also be computed with a uni-dimensional recurrence:\n:<math>\\binom n k = \\binom n{k-1}(n-k+1)/k,</math>\nwith the initial value <math display = inline>\\binom n 0 =1</math> (The division is not displayed as a fraction for emphasizing that it must be computed after the multiplication, for not introducing fractional numbers).\nThis recurrence is widely used in computers because it does not require to build a table as does the bi-dimensional recurrence, and does not involve very large integers as does the formula with factorials (if one uses <math display = inline>\\binom nk= \\binom n{n-k}, </math> all involved integers are smaller than the final result).\n\n== Difference operator and difference equations {{anchor|Relationship to difference equations narrowly defined}} ==\n\nThe '''{{vanchor|difference operator}}''' is an [[operator (mathematics)|operator]] that maps [[sequence]]s to sequences, and, more generally, [[function (mathematics)|functions]] to functions. It is commonly denoted <math>\\Delta,</math> and is defined, in [[functional notation]], as \n:<math>(\\Delta f)(x)=f(x+1)-f(x).</math>\nIt is thus a special case of [[finite difference]].\n\nWhen using the index notation for sequences, the definition becomes\n:<math>(\\Delta a)_n= a_{n+1} - a_n.</math>\nThe parentheses around <math>\\Delta f</math> and <math>\\Delta a</math> are generally omitted, and <math>\\Delta a_n</math> must be understood as the term of index {{mvar|n}} in the sequence <math>\\Delta a,</math> and not <math>\\Delta</math> applied to the element <math>a_n.</math>\n\nGiven [[sequence]] <math>a=(a_n)_{n\\in \\N},</math> the '''{{vanchor|first difference}}''' of {{mvar|a}} is <math>\\Delta a.</math>\n\nThe '''{{vanchor|second difference}}''' is \n<math>\\Delta^2 a=(\\Delta\\circ\\Delta)a= \\Delta(\\Delta a).</math> A simple computation shows that\n:<math>\\Delta^2 a_n= a_{n+2} - 2a_{n+1} + a_n.</math>\n\nMore generally: the  {{mvar|k}}''th difference'' is defined recursively as <math>\\Delta^k=\\Delta\\circ \\Delta^{k-1},</math> and one has\n:<math>\\Delta^k a_n = \\sum_{t=0}^k (-1)^t \\binom{k}{t} a_{n+k-t}.</math>\n\nThis relation can be inverted, giving\n:<math>a_{n+k} = a_n + {k\\choose 1} \\Delta a_n  + \\cdots + {k\\choose k} \\Delta^k(a_n).</math>\n\nA '''{{vanchor|difference equation}}''' of order {{mvar|k}} is an equation that involves the {{mvar|k}} first differences of a sequence or a function, in the same way as a [[ordinary differential equation|differential equation]] of order {{mvar|k}} relates the {{mvar|k}} first [[derivative]]s of a function.\n\nThe two above relations allow transforming a recurrence relation of order {{mvar|k}} into a difference equation of order {{mvar|k}}, and, conversely, a difference equation of order {{mvar|k}} into recurrence relation of order {{mvar|k}}. Each transformation is the [[inverse function|inverse]] of the other, and the sequences that are solution of the difference equation are exactly those that satisfies the recurrence relation.\n\nFor example, the difference equation\n:<math>3\\Delta^2 a_n + 2\\Delta a_n + 7a_n = 0</math>\nis equivalent to the recurrence relation\n:<math>3a_{n+2} = 4a_{n+1} - 8a_n,</math>\nin the sense that the two equations are satisfied by the same sequences.\n\nAs it is equivalent for a sequence to satisfy a recurrence relation or to be the solution of a difference equation, the use of the term \"difference equation\" is not limited to equations using a difference operator,<ref>C. R. Wylie, ''Advanced Engineering Mathematics'' (1960) page 167. \"However, in the study of difference equations we do not ordinarily consider equations of the form ''f''(Δ)''y'' = 𝜙(''x'') ... but rather equations of the form ''f''(''E'')''y'' = 𝜙(''x'')\" where Δ is the difference operator and ''E'' is a [[shift operator]].</ref> and the two terms \"recurrence relation\" and \"difference equation\" can be used interchangeably.<ref>J. Bradley, ''Introduction to Discrete Mathematics'' (1988) page 266. \"Older texts on this topic tend to talk primarily about difference equations; newer ones talk about recurrence equations or relations. This reflects an important change in mathematical thinking since the 1950s; difference equations are primarily seen as an approximation of differential equations, a calculus topic. Recurrence equations are seen as an important topic in their own right. The shift in names suggests the growing recognition of the importance of discrete mathematics.\"</ref> See [[Rational difference equation]], [[Z-transform#Linear constant-coefficient difference equation|Linear constant-coefficient difference equation]] and [[Matrix difference equation]] for examples of using \"difference equation\" instead of \"recurrence relation\".\n\nDifference equations resemble differential equations, and this resemblance is often used to mimic methods for solving differentiable equations to apply to solving difference equations, and therefore recurrence relations.\n\n[[Summation equation]]s relate to difference equations as [[integral equation]]s relate to differential equations. See [[time scale calculus]] for a unification of the theory of difference equations with that of differential equations.\n\n===From sequences to grids===\nSingle-variable or one-dimensional recurrence relations are about sequences (i.e. functions defined on one-dimensional grids). Multi-variable or n-dimensional recurrence relations are about <math>n</math>-dimensional grids. Functions defined on <math>n</math>-grids can also be studied with partial difference equations.<ref>[https://books.google.com/books?id=1klnDGelHGEC Partial difference equations], Sui Sun Cheng, CRC Press, 2003, {{isbn|978-0-415-29884-1}}</ref>","x":-2471,"y":-262,"width":250,"height":462},
		{"id":"5d6dd3959171bc8c","type":"text","text":"State Engine\n---\n===Integro-differential equation===\n{{Main|Integro-differential equation}}\nIntegro-differential equations of Volterra type are functional differential equations with continuous argument values.<ref name=\":0\" /> Integro-differential equations involve both the integrals and derivatives of some function with respect to its argument.\n\nThe continuous integro-differential equation for retarded functional differential equations, <math>x'(t)=f\\bigl(t, x(t-\\tau_1(t)), x(t-\\tau_2(t)), \\ldots,x(t-\\tau_k(t)) \\bigr)</math>, can be written as \n:<math>x'(t)=f\\Biggl(t,\\int_{t-\\tau(t)}^t K(t, \\theta,x(\\theta)) \\,\\mathrm{d}\\theta \\Biggr), \\quad \\tau(t) \\geq0</math>\n\n","x":-2324,"y":-739,"width":250,"height":259},
		{"id":"667d7048d88b924f","type":"text","text":"#  Functional differential equation\n---\n\nA '''functional differential equation''' is a [[differential equation]] with deviating argument. That is, a functional differential equation is an equation that contains a function and some of its derivatives evaluated at different argument values.<ref name=\":0\">{{Cite book|title=Applied Theory of Functional Differential Equations|last1=Kolmanovskii|first1=V.| last2=Myshkis| first2=A.| publisher=Kluwer Academic Publishers|year=1992|isbn=0-7923-2013-1|location=The Netherlands|author2-link=Myshkis, Anatoliĭ Dmitrievich}}</ref>\n\nFunctional differential equations find use in mathematical models that assume a specified behavior or phenomenon depends on the present as well as the past state of a system.<ref name=\":3\">{{Cite book|title=Functional Differential Equations|last=Hale| first=Jack K.| publisher=Springer-Verlag|year=1971|isbn=0-387-90023-3|location=United States}}</ref> In other words, past events explicitly influence future results. For this reason, functional differential equations are more applicable than [[Ordinary differential equation|ordinary differential equations (ODE)]], in which future behavior only implicitly depends on the past.\n\n==Definition==\nUnlike ordinary differential equations, which contain a function of one variable and its derivatives evaluated with the same input, functional differential equations contain a function and its derivatives evaluated with different input values.\n\n*An example of an ordinary differential equation would be <math>f'(x) = 2f(x) +1</math>\n*In comparison, a functional differential equation would be <math>f'(x)=2f(x+3)-[f(x-1)]^2</math>\n\nThe simplest type of functional differential equation called the '''retarded functional differential equation''' or '''retarded differential difference equation''', is of the form<ref name=\":1\">{{Cite book|title=Introduction to Functional Differential Equations| last1=Hale|first1=Jack K.|last2=Verduyn Lunel|first2=Sjoerd M.|publisher=Springer-Verlag|year=1993| isbn=0-387-94076-6| location=United States}}</ref>\n\n:<math>x'(t) = f\\bigl(t, x(t),x(t-r)\\bigr)</math>\n\n===Examples===\nA simple functional differential equation is the linear first-order [[delay differential equation]]<ref name=\":4\">{{Cite web|last=Falbo|first=Clement E.|title=Some Elementary Methods for Solving Functional Differential Equations| url=http://www.mathfile.net/hicstat_FDE.pdf|archive-url=https://web.archive.org/web/20161220114251/http://www.mathfile.net/hicstat_FDE.pdf |archive-date=2016-12-20 }}</ref>{{unreliable source?|date=June 2021}} which is given by\n:<math>x'(t)=\\alpha_1 x(t) + \\alpha_2 x(t-\\tau)+f(t), t\\geq0</math>\nwhere <math>\\alpha_1, \\alpha_2, \\tau</math> are constants, <math>f(t)</math> is some [[continuous function]], and <math>x</math> is a scalar. Below is a table with a comparison of several ordinary and functional differential equations.\n{| class=\"wikitable\" style=\"margin-left: auto; margin-right: auto; border: none;\"\n!\n!Ordinary differential equation\n!Functional differential equation\n|-\n| rowspan=\"4\" | '''Examples'''\n| <math>f'(x)=x^2-3</math>\n|<math>f'(x) = 3x-f(x-4)</math>\n|-\n|<math>f'(x)=f(x)-8</math>\n|<math>x'(t) = 3x(2t)-\\bigl[x(t-1)\\bigr]^2</math>\n|-\n|<math>F\\bigl(t, x(t), x'(t),x''(t)\\bigr)=0</math>\n|<math>2x(3t+1)-5x(4t)=1</math>\n|-\n|<math>f'(x) = 4f(x) -3x</math>\n|\n|}\n\n== Types of functional differential equations ==\n\"Functional differential equation\" is the general name for a number of more specific types of differential equations that are used in numerous applications.\n\n=== Differential difference equation ===\nDifferential difference equations are functional differential equations in which the argument values are discrete.<ref name=\":0\" /> The general form for functional differential equations of finitely many discrete deviating arguments is \n:<math>x^{(n)}(t)=f\\Bigl(t, x^{(n_1)}\\bigl(t-\\tau_1(t)\\bigr), x^{(n_2)}\\bigl(t-\\tau_2(t)\\bigr),\\ldots,x^{(n_k)}\\bigl(t-\\tau_k(t)\\bigr)\\Bigr) </math>\nwhere <math>x(t)\\in \\R^m,\\, n_1, n_2,\\ldots,n_i \\geq0, </math> and <math>\\tau_1(t),\\tau_2(t),\\ldots,\\tau_i(t) \\geq 0</math>\n\nDifferential difference equations are also referred to as ''retarded'', ''neutral'', ''advanced'', and ''mixed'' functional differential equations. This classification depends on whether the rate of change of the current state of the system depends on past values, future values, or both.<ref name=\":5\">{{Cite book|title=Bifurcation Theory of Functional Differential Equations| last1=Guo|first1=S.|last2=Wu|first2=J.|publisher=Springer|year=2013|isbn=978-1-4614-6991-9|location=New York|pages=41–60}}</ref>\n{| class=\"wikitable\" style=\"margin-left: auto; margin-right: auto; border: none;\"\n! colspan=\"2\" |Classifications of differential difference equations<ref>{{Cite book|title=Differential-Difference Equations| url=https://archive.org/details/differentialdiff00rbel|url-access=limited|last1=Bellman|first1=Richard|last2=Cooke|first2=Kenneth L.| publisher=Academic Press|year=1963|isbn=978-0124109735|location=New York, NY|pages=[https://archive.org/details/differentialdiff00rbel/page/n56 42]–49}}</ref>\n|-\n|Retarded\n|<math>x'(t)=f\\bigl(t,x(t),x(t-\\tau)\\bigr)</math>\n|-\n|Neutral\n|<math>x'(t)=f\\bigl(t, x(t),x(t-\\tau),x'(t-\\tau)\\bigr)</math>\n|-\n|Advanced\n|<math>x'(t-\\tau)=f\\bigl(t,x(t),x(t-\\tau)\\bigr)</math>\n|}\n\n==== Delay differential equation ====\n{{Main|Delay differential equation}}\nFunctional differential equations of retarded type occur when <math>\\max \\{n_1, n_2,\\ldots,n_k\\ \\}<n</math> for the equation given above. In other words, this class of functional differential equations depends on the past and present values of the function with delays.\n\nA simple example of a retarded functional differential equation is \n:<math>x'(t)=-x(t-\\tau)</math>\nwhereas a more general form for discrete deviating arguments can be written as\n:<math>x'(t)=f\\Bigl(t,x\\bigl(t-\\tau_1(t)\\bigr), x\\bigl(t-\\tau_2(t)\\bigr),\\ldots,x\\bigl(t-\\tau_k(t)\\bigr)\\Bigr).</math>\n\n==== Neutral differential equations ====\nFunctional differential equations of neutral type, or neutral differential equations occur when\n:<math>\\max\\{n_1, n_2,\\ldots,n_k\\}=n.</math>\nNeutral differential equations depend on past and present values of the function, similarly to retarded differential equations, except it also depends on derivatives with delays. In other words, retarded differential equations do not involve the given function's derivative with delays while neutral differential equations do.\n\n===Integro-differential equation===\n{{Main|Integro-differential equation}}\nIntegro-differential equations of Volterra type are functional differential equations with continuous argument values.<ref name=\":0\" /> Integro-differential equations involve both the integrals and derivatives of some function with respect to its argument.\n\nThe continuous integro-differential equation for retarded functional differential equations, <math>x'(t)=f\\bigl(t, x(t-\\tau_1(t)), x(t-\\tau_2(t)), \\ldots,x(t-\\tau_k(t)) \\bigr)</math>, can be written as \n:<math>x'(t)=f\\Biggl(t,\\int_{t-\\tau(t)}^t K(t, \\theta,x(\\theta)) \\,\\mathrm{d}\\theta \\Biggr), \\quad \\tau(t) \\geq0</math>\n","x":-1911,"y":-811,"width":250,"height":331},
		{"id":"d8865c9486f74513","type":"text","text":"Integro-differential equation\n---\n{{Short description|Equation involving both integrals and derivatives of a function}}\n{{Differential equations}}\n{{context|date=May 2025}}\nIn [[mathematics]], an '''integro-differential equation''' is an [[equation]] that involves both [[integral]]s and [[derivative]]s of a [[function (mathematics)|function]].\n\n==General first order linear equations==\n\nThe general first-order, linear (only with respect to the term involving derivative) integro-differential equation is of the form\n\n:<math>\n\\frac{d}{dx}u(x) + \\int_{x_0}^x f(t,u(t))\\,dt = g(x,u(x)), \\qquad u(x_0) = u_0, \\qquad x_0 \\ge 0.\n</math>\n\nAs is typical with [[differential equations]], obtaining a closed-form solution can often be difficult. In the relatively few cases where a solution can be found, it is often by some kind of integral transform, where the problem is first transformed into an algebraic setting. In such situations, the solution of the problem may be derived by applying the inverse transform to the solution of this algebraic equation.\n\n===Example===\n\nConsider the following second-order problem,\n\n: <math>\nu'(x) + 2u(x) + 5\\int_{0}^{x}u(t)\\,dt = \\theta(x)\n \\qquad \\text{with} \\qquad u(0)=0,\n</math>\n\nwhere\n\n: <math>\n \\theta(x) = \\left\\{ \\begin{array}{ll}\n         1, \\qquad x \\geq 0\\\\\n         0, \\qquad x < 0 \\end{array} \n\\right.\n</math>\n\nis the [[Heaviside step function]]. The [[Laplace transform]] is defined by,\n\n:<math> U(s) = \\mathcal{L} \\left\\{u(x)\\right\\}=\\int_0^{\\infty} e^{-sx} u(x) \\,dx. </math>\n\nUpon taking term-by-term Laplace transforms, and utilising the rules for derivatives and integrals, the integro-differential equation is converted into the following algebraic equation,\n\n:<math> s U(s) - u(0) + 2U(s) + \\frac{5}{s}U(s) = \\frac{1}{s}. </math>\n\nThus,\n\n:<math> U(s) = \\frac{1}{s^2 + 2s + 5} </math>.\n\nInverting the Laplace transform using [[Methods of contour integration|contour integral methods]] then gives\n\n:<math> u(x) = \\frac{1}{2} e^{-x} \\sin(2x) \\theta(x) </math>.\n\nAlternatively, one can [[complete the square]] and use a table of [[List of Laplace transforms#Table|Laplace transforms]] (\"exponentially decaying sine wave\") or recall from memory to proceed:\n\n:<math> U(s) = \\frac{1}{s^2 + 2s + 5} = \\frac{1}{2} \\frac{2}{(s+1)^2+4} \\Rightarrow u(x) = \\mathcal L^{-1}\\left\\{ U(s) \\right\\} = \\frac{1}{2} e^{-x} \\sin(2x) \\theta(x) </math>.\n","x":-2449,"y":-1577,"width":250,"height":503},
		{"id":"18449e5355cade59","type":"text","text":"# Integral element\n---\nIn [[commutative algebra]], an element ''b'' of a [[commutative ring]] ''B'' is said to be '''integral over''' a [[subring]] ''A'' of ''B'' if ''b'' is a [[root of a polynomial|root]] of some [[monic polynomial]] over ''A''.<ref>The above equation is sometimes called an integral equation and ''b'' is said to be integrally dependent on ''A'' (as opposed to [[algebraic dependent]]).</ref>\n\nIf ''A'', ''B'' are [[field (mathematics)|fields]], then the notions of \"integral over\" and of an \"integral extension\" are precisely \"[[algebraic element|algebraic]] over\" and \"[[algebraic extension]]s\" in [[field theory (mathematics)|field theory]] (since the root of any [[polynomial]] is the root of a monic polynomial).\n\nThe case of greatest interest in [[number theory]] is that of [[complex numbers]] integral over '''Z''' (e.g., <math>\\sqrt{2}</math> or <math>1+i</math>); in this context, the integral elements are usually called [[algebraic integer]]s.  The algebraic integers in a finite [[field extension|extension field]] ''k'' of the [[rational number|rationals]] '''Q''' form a subring of ''k'', called the [[ring of integers]] of ''k'', a central object of study in [[algebraic number theory]].\n\nIn this article, the term ''[[ring (mathematics)|ring]]'' will be understood to mean ''commutative ring'' with a multiplicative identity.\n\n==Definition==\nLet <math>B</math> be a ring and let <math>A \\subset B</math> be a subring of <math>B.</math>\nAn element <math>b</math> of <math>B</math> is said to be '''integral over''' <math>A</math> if for some <math>n \\geq 1,</math> there exists <math>a_0,\\ a_1, \\ \\dots,\\ a_{n-1}</math> in  <math>A</math> such that\n<math display=\"block\">b^n + a_{n-1} b^{n-1} + \\cdots + a_1 b + a_0 = 0.</math>\n\nThe set of elements of <math>B</math> that are integral over <math>A</math> is called the '''integral closure''' of <math>A</math> in <math>B.</math> The integral closure of any subring <math>A</math> in <math>B</math> is, itself, a subring of <math>B</math> and contains <math>A.</math> If every element of <math>B</math> is integral over <math>A,</math> then we say that <math>B</math> is '''integral over''' <math>A</math>, or equivalently <math>B</math> is an '''integral extension''' of <math>A.</math>\n\n==Examples==\n\n=== Integral closure in algebraic number theory ===\nThere are many examples of integral closure which can be found in algebraic number theory since it is fundamental for defining the [[ring of integers]] for an [[Algebraic extension|algebraic field extension]] <math>K/\\mathbb{Q}</math> (or <math>L/\\mathbb{Q}_p</math>).\n\n==== Integral closure of integers in rationals ====\n[[Integer]]s are the only elements of '''Q''' that are integral over '''Z'''. In other words, '''Z''' is the integral closure of '''Z''' in '''Q'''.\n\n==== Quadratic extensions ====\nThe [[Gaussian integer]]s are the complex numbers of the form <math>a + b \\sqrt{-1},\\, a, b \\in \\mathbf{Z}</math>, and are integral over '''Z'''. <math>\\mathbf{Z}[\\sqrt{-1}]</math> is then the integral closure of '''Z''' in <math>\\mathbf{Q}(\\sqrt{-1})</math>. Typically this ring is denoted <math>\\mathcal{O}_{\\mathbb{Q}[i]}</math>.\n\nThe integral closure of '''Z''' in <math>\\mathbf{Q}(\\sqrt{5})</math> is the ring\n:<math>\\mathcal{O}_{\\mathbb{Q}[\\sqrt{5}]} = \\mathbb{Z}\\!\\left[ \\frac{1 + \\sqrt{5}}{2} \\right]</math>\nThis example and the previous one are examples of [[quadratic integer]]s. The integral closure of a quadratic extension <math>\\mathbb{Q}(\\sqrt{d})</math> can be found by constructing the [[Minimal polynomial (field theory)|minimal polynomial]] of an arbitrary element <math>a + b \\sqrt{d}</math> and finding number-theoretic criterion for the polynomial to have integral coefficients. This analysis can be found in the [[Quadratic integer#Determining the ring of integers|quadratic extensions article]].\n\n==== Roots of unity ====\nLet ζ be a [[root of unity]]. Then the integral closure of '''Z''' in the [[cyclotomic field]] '''Q'''(ζ) is '''Z'''[ζ].<ref>{{harvnb|Milne|2020|loc=Theorem 6.4}}</ref> This can be found by using the [[Minimal polynomial (field theory)|minimal polynomial]] and using [[Eisenstein's criterion]].\n\n==== Ring of algebraic integers ====\nThe integral closure of '''Z''' in the field of complex numbers '''C''', or the algebraic closure <math>\\overline{\\mathbb{Q}}</math> is called the ''ring of [[algebraic integer]]s''.\n\n==== Other ====\nThe [[roots of unity]], [[nilpotent element]]s and [[idempotent (ring theory)|idempotent element]]s in any ring are integral over '''Z'''.\n\n=== Integral closure in algebraic geometry ===\nIn [[geometry]], integral closure is closely related with [[Noether normalization lemma|normalization]] and [[normal scheme]]s. It is the first step in [[resolution of singularities]] since it gives a process for resolving singularities of codimension 1.\n\n* For example, the integral closure of <math>\\mathbb{C}[x,y,z]/(xy)</math> is the ring <math>\\mathbb{C}[x,z] \\times \\mathbb{C}[y,z]</math> since geometrically, the first ring corresponds to the <math>xz</math>-plane unioned with the <math>yz</math>-plane. They have a codimension 1 singularity along the <math>z</math>-axis where they intersect.\n*Let a [[finite group]] ''G'' [[group action|act]] on a ring ''A''. Then ''A'' is integral over ''A''<sup>''G''</sup>, the set of elements fixed by ''G''; see [[Fixed-point subring|Ring of invariants]].\n*Let ''R'' be a ring and ''u'' a [[unit (ring theory)|unit]] in a ring containing ''R''. Then<ref>{{harvnb|Kaplansky|1974|loc=1.2. Exercise 4.}}</ref>\n\n#''u''<sup>−1</sup> is integral over ''R'' [[if and only if]] ''u''<sup>−1</sup> ∈ ''R''[''u''].\n#<math>R[u] \\cap  R[u^{-1}]</math> is integral over ''R''.\n#The integral closure of the [[homogeneous coordinate ring]] of a normal [[projective variety]] ''X'' is the [[ring of sections]]<ref>{{harvnb|Hartshorne|1977|loc=Ch. II, Exercise 5.14}}</ref>\n\n::<math>\\bigoplus_{n \\ge 0} \\operatorname{H}^0(X, \\mathcal{O}_X(n)).</math>\n\n=== Integrality in algebra ===\n\n* If <math>\\overline{k}</math> is an [[algebraic closure]] of a field ''k'', then <math>\\overline{k}[x_1, \\dots, x_n]</math> is integral over <math>k[x_1, \\dots, x_n].</math>\n* The integral closure of '''C'''<nowiki>[[</nowiki>''x''<nowiki>]]</nowiki> in a finite extension of '''C'''((''x'')) is of the form <math>\\mathbf{C}[[x^{1/n}]]</math> (cf. [[Puiseux series]]){{citation needed|date=October 2012}}\n","x":-2917,"y":-1935,"width":250,"height":358},
		{"id":"cb72f58d8c63c48f","type":"text","text":"8-Tuple Proof\n---\n=== Example 2 ===\nConsider a more complex set of assumptions: \"It is not sunny today and it is colder than yesterday\". \"We will go swimming only if it is sunny\", \"If we do not go swimming, then we will have a barbecue\", and \"If we will have a barbecue, then we will be home by sunset\" lead to the conclusion \"We will be home by sunset.\"\nProof by rules of inference: Let <math>p</math> be the proposition \"It is sunny today\", <math>q</math> the proposition \"It is colder than yesterday\", <math>r</math> the proposition \"We will go swimming\", <math>s</math> the proposition \"We will have a barbecue\", and <math>t</math> the proposition \"We will be home by sunset\". Then the hypotheses become <math>\\neg p \\wedge q, r \\rightarrow p, \\neg r \\rightarrow s</math> and <math>s \\rightarrow t</math>. Using our intuition we conjecture that the conclusion might be <math>t</math>. Using the Rules of Inference table we can prove the conjecture easily:\n{| class=\"wikitable\"\n|-\n! Step\n! Reason\n|-\n| 1.<math>\\neg p \\wedge q</math>\n| [[Hypothesis]]\n|-\n| 2. <math>\\neg p</math>\n| [[Conjunction elimination|Simplification]] using Step 1\n|-\n| 3. <math>r \\rightarrow p</math>\n| [[Hypothesis]]\n|-\n| 4. <math>\\neg r</math>\n| [[Modus tollens]] using Step 2 and 3\n|-\n| 5. <math>\\neg r \\rightarrow s</math>\n| [[Hypothesis]]\n|-\n| 6. <math>s</math>\n| [[Modus ponens]] using Step 4 and 5\n|-\n| 7. <math>s \\rightarrow t</math>\n| [[Hypothesis]]\n|-\n| 8. <math>t</math>\n| [[Modus ponens]] using Step 6 and 7\n|}\n","x":-2975,"y":-839,"width":250,"height":488},
		{"id":"48a123d2254fb3a5","type":"text","text":"# Partition topology\n---\nIn [[mathematics]], a '''partition topology''' is a [[topological space|topology]] that can be induced on any set <math>X</math> by [[Partition of a set|partitioning]] <math>X</math> into disjoint subsets <math>P;</math> these subsets form the [[basis (topology)|basis]] for the topology. There are two important examples which have their own names:\n* The '''{{visible anchor|odd–even topology}}''' is the topology where <math>X = \\N</math> and <math>P = {\\left\\{~\\{2k-1, 2k\\} : k \\in \\N\\right\\} }.</math> Equivalently, <math>P = \\{~ \\{1,2\\}, \\{3,4\\},\\{5,6\\}, \\ldots\\}.</math>\n* The '''{{visible anchor|deleted integer topology}}''' is defined by letting <math>X = \\begin{matrix} \\bigcup_{n \\in \\N} (n-1,n) \\subseteq \\Reals \\end{matrix}</math> and <math>P = {\\left\\{(0,1), (1,2), (2,3), \\ldots\\right\\} }.</math>\n\nThe trivial partitions yield the [[discrete topology]] (each point of <math>X</math> is a set in <math>P,</math> so <math>P = \\{~ \\{x\\} ~ : ~ x \\in X ~\\}</math>) or [[indiscrete topology]] (the entire set <math>X</math> is in <math>P,</math> so <math>P = \\{X\\}</math>).  \n\nAny set <math>X</math> with a partition topology generated by a partition <math>P</math> can be viewed as a [[pseudometric space]] with a pseudometric given by: \n<math display=block>d(x, y) = \\begin{cases} 0 & \\text{if } x \\text{ and } y \\text{ are in the same partition element} \\\\\n1 & \\text{otherwise}.\n\\end{cases}</math>\n\nThis is not a [[Metric (mathematics)|metric]] unless <math>P</math> yields the discrete topology.\n\nThe partition topology provides an important example of the independence of various [[separation axioms]]. Unless <math>P</math> is trivial, at least one set in <math>P</math> contains more than one point, and the elements of this set are [[topologically indistinguishable]]: the topology does not separate points. Hence <math>X</math> is not a [[Kolmogorov space]], nor a [[T1 space|T<sub>1</sub> space]], a [[Hausdorff space]] or an [[Urysohn and completely Hausdorff spaces|Urysohn space]]. In a partition topology the complement of every open set is also open, and therefore a set is open if and only if it is closed. Therefore, <math>X</math> is [[regular space|regular]], [[completely regular space|completely regular]], [[normal space|normal]] and [[completely normal space|completely normal]]. <math>X / P</math> is the discrete topology.\n\n==See also==\n\n* {{annotated link|List of topologies}}\n\n==References==\n\n{{reflist}}\n\n{{nofootnotes|date=April 2020}}\n* {{Citation | last1=Steen | first1=Lynn Arthur | author1-link=Lynn Arthur Steen | last2=Seebach | first2=J. Arthur Jr. | author2-link=J. Arthur Seebach, Jr. | title=[[Counterexamples in Topology]] | orig-date=1978 | publisher=[[Springer-Verlag]] | location=Berlin, New York | edition=[[Dover Publications|Dover]] reprint of 1978 | isbn=978-0-486-68735-3 |mr=507446 | year=1995}}\n\n[[Category:Topological spaces]]\n\n{{topology-stub}}\n\n","x":-3041,"y":-262,"width":250,"height":395},
		{"id":"65cd3cb91ac87314","type":"text","text":"# Measure-preserving dynamical system\n--\n{{Short description|Subject of study in ergodic theory}}\n{{redirect|Area-preserving map|the map projection concept|Equal-area map}}\nIn [[mathematics]], a '''measure-preserving dynamical system''' is an object of study in the abstract formulation of [[dynamical systems]], and [[ergodic theory]] in particular. Measure-preserving systems obey the [[Poincaré recurrence theorem]], and are a special case of [[conservative system]]s. They provide the formal, mathematical basis for a broad range of physical systems, and, in particular, many systems from [[classical mechanics]] (in particular, most [[dissipative system|non-dissipative]] systems) as well as systems in [[thermodynamic equilibrium]].\n\n==Definition==\nA measure-preserving dynamical system is defined as a [[probability space]] and a [[Invariant measure|measure-preserving]] transformation on it. In more detail, it is a system\n\n:<math>(X, \\mathcal{B}, \\mu, T)</math>\n\nwith the following structure:\n\n*<math>X</math> is a set,\n*<math>\\mathcal B</math> is a [[sigma-algebra|&sigma;-algebra]] over <math>X</math>,\n*<math>\\mu:\\mathcal{B}\\rightarrow[0,1]</math> is a [[probability measure]], so that <math>\\mu (X) = 1</math>, and <math>\\mu(\\varnothing) = 0</math>,\n*<math> T:X \\rightarrow X</math> is a [[measurable function|measurable]] transformation which [[Invariant measure|preserves]] the measure <math>\\mu</math>, i.e., <math>\\forall A\\in \\mathcal{B}\\;\\; \\mu(T^{-1}(A))=\\mu(A) </math>.\n\n==Discussion==\nOne may ask why the measure preserving transformation is defined in terms of the inverse <math>\\mu(T^{-1}(A))=\\mu(A)</math> instead of the forward transformation <math>\\mu(T(A))=\\mu(A)</math>. This can be understood intuitively. \n\nConsider the typical measure on the unit interval <math>[0, 1]</math>, and a map <math>Tx = 2x\\mod 1 = \\begin{cases}\n2x \\text{ if } x < 1/2 \\\\\n2x-1 \\text{ if } x > 1/2 \\\\\n\\end{cases}</math>. This is the [[Bernoulli map]]. Now, distribute an even layer of paint on the unit interval <math>[0, 1]</math>, and then map the paint forward. The paint on the <math>[0, 1/2]</math> half is spread thinly over all of <math>[0, 1]</math>, and the paint on the <math>[1/2, 1]</math> half as well. The two layers of thin paint, layered together, recreates the exact same paint thickness.\n\nMore generally, the paint that would arrive at subset <math>A \\subset [0, 1]</math> comes from the subset <math>T^{-1}(A)</math>. For the paint thickness to remain unchanged (measure-preserving), the mass of incoming paint should be the same: <math>\\mu(A) = \\mu(T^{-1}(A))</math>.\n\nConsider a mapping <math>\\mathcal{T}</math> of [[power set]]s:\n:<math>\\mathcal{T}:P(X)\\to P(X)</math>\nConsider now the special case of maps <math>\\mathcal{T}</math> which preserve intersections, unions and complements (so that it is a map of [[Borel set]]s) and also sends <math>X</math> to <math>X</math> (because we want it to be [[conservative system|conservative]]). Every such conservative, Borel-preserving map can be specified by some [[surjective]] map <math>T:X\\to X</math> by writing <math>\\mathcal{T}(A)=T^{-1}(A)</math>. Of course, one could also define <math>\\mathcal{T}(A)=T(A)</math>, but this is not enough to specify all such possible maps <math>\\mathcal{T}</math>. That is, conservative, Borel-preserving maps <math>\\mathcal{T}</math> cannot, in general, be written in the form <math>\\mathcal{T}(A)=T(A);</math>.\n\n<math>\\mu(T^{-1}(A))</math> has the form of a [[Pushforward measure|pushforward]], whereas <math>\\mu(T(A))</math> is generically called a [[pullback]]. Almost all properties and behaviors of dynamical systems are defined in terms of the pushforward. For example, the [[transfer operator]] is defined in terms of the pushforward of the transformation map <math>T</math>; the measure <math>\\mu</math> can now be understood as an [[invariant measure]]; it is just the [[Perron–Frobenius theorem|Frobenius–Perron eigenvector]] of the transfer operator (recall, the FP eigenvector is the largest eigenvector of a matrix; in this case it is the eigenvector which has the eigenvalue one: the invariant measure.)\n\nThere are two classification problems of interest. One, discussed below, fixes <math>(X, \\mathcal{B}, \\mu)</math> and asks about the isomorphism classes of a transformation map <math>T</math>.  The other, discussed in [[transfer operator]], fixes <math>(X, \\mathcal{B})</math> and <math>T</math>, and asks about maps <math>\\mu</math> that are measure-like. Measure-like, in that they preserve the Borel properties, but are no longer invariant; they are in general dissipative and so give insights into [[dissipative system]]s and the route to equilibrium.\n\nIn terms of physics, the measure-preserving dynamical system <math>(X, \\mathcal{B}, \\mu, T)</math> often describes a physical system that is in equilibrium, for example, [[thermodynamic equilibrium]]. One might ask: how did it get that way? Often, the answer is by stirring, [[mixing (mathematics)|mixing]], [[turbulence]], [[thermalization]] or other such processes. If a transformation map <math>T</math> describes this stirring, mixing, etc. then the system <math>(X, \\mathcal{B}, \\mu, T)</math> is all that is left, after all of the transient modes have decayed away. The transient modes are precisely those eigenvectors of the transfer operator that have eigenvalue less than one; the invariant measure <math>\\mu</math> is the one mode that does not decay away. The rate of decay of the transient modes are given by (the logarithm of) their eigenvalues; the eigenvalue one corresponds to infinite half-life.\n\n==Informal example==\nThe [[microcanonical ensemble]] from physics provides an informal example. Consider, for example, a fluid, gas or plasma in a box of width, length and height <math>w\\times l\\times h,</math> consisting of <math>N</math> atoms. A single atom in that box might be anywhere, having arbitrary velocity; it would be represented by a single point in <math>w\\times l\\times h\\times \\mathbb{R}^3.</math> A given collection of <math>N</math> atoms would then be a ''single point'' somewhere in the space <math>(w\\times l\\times h)^N \\times \\mathbb{R}^{3N}.</math> The \"ensemble\" is the collection of all such points, that is, the collection of all such possible boxes (of which there are an uncountably-infinite number). This ensemble of all-possible-boxes is the space <math>X</math> above.\n\nIn the case of an [[ideal gas]], the measure <math>\\mu</math> is given by the [[Maxwell–Boltzmann distribution]]. It is a [[product measure]], in that if <math>p_i(x,y,z,v_x,v_y,v_z)\\,d^3x\\,d^3p</math> is the probability of atom <math>i</math> having position and velocity <math>x,y,z,v_x,v_y,v_z</math>, then, for <math>N</math> atoms, the probability is the product of <math>N</math> of these. This measure is understood to apply to the ensemble. So, for example, one of the possible boxes in the ensemble has all of the atoms on one side of the box. One can compute the likelihood of this, in the Maxwell–Boltzmann measure. It will be enormously tiny, of order <math>\\mathcal{O}\\left(2^{-3N}\\right).</math>  Of all possible boxes in the ensemble, this is a ridiculously small fraction.\n\nThe only reason that this is an \"informal example\" is because writing down the transition function <math>T</math> is difficult, and, even if written down, it is hard to perform practical computations with it. Difficulties are compounded if there are interactions between the particles themselves, like a [[Van der Waals force|van der Waals interaction]] or some other interaction suitable for a liquid or a plasma; in such cases, the invariant measure is no longer the Maxwell–Boltzmann distribution. The art of physics is finding reasonable approximations.\n\nThis system does exhibit one key idea from the classification of measure-preserving dynamical systems: two ensembles, having different temperatures, are inequivalent. The entropy for a given canonical ensemble depends on its temperature; as physical systems, it is \"obvious\" that when the temperatures differ, so do the systems. This holds in general: systems with different entropy are not isomorphic.\n\n==Examples==\n[[Image:Exampleergodicmap.svg|thumb|Example of a ([[Lebesgue measure]]) preserving map: ''T'' : [0,1) → [0,1), <math>x \\mapsto 2x \\mod 1.</math>]] Unlike the informal example above, the examples below are sufficiently well-defined and tractable that explicit, formal computations can be performed.\n\n* μ could be the normalized angle measure dθ/2π on the [[unit circle]], and ''T'' a rotation. See [[equidistribution theorem]];\n* the [[Bernoulli scheme]];\n* the [[interval exchange transformation]];\n* with the definition of an appropriate measure, a [[subshift of finite type]];\n* the [[base flow (random dynamical systems)|base flow]] of a [[random dynamical system]];\n* the flow of a Hamiltonian vector field on the tangent bundle of a closed connected smooth manifold is measure-preserving (using the measure induced on the Borel sets by the [[Volume form#Symplectic manifolds|symplectic volume form]]) by [[Liouville's theorem (Hamiltonian)]];<ref name=walters2000>{{cite book |last=Walters |first=Peter |title=An Introduction to Ergodic Theory |year=2000 |publisher=Springer |isbn=0-387-95152-0 }}</ref>\n* for certain maps and [[Markov chain|Markov processes]], the [[Krylov–Bogolyubov theorem]] establishes the existence of a suitable measure to form a measure-preserving dynamical system.\n\n==Generalization to groups and monoids==\nThe definition of a measure-preserving dynamical system can be generalized to the case in which ''T'' is not a single transformation that is iterated to give the dynamics of the system, but instead is a [[monoid]] (or even a [[group (mathematics)|group]], in which case we have the [[Group action|action of a group]] upon the given probability space) of transformations ''T<sub>s</sub>'' : ''X'' → ''X'' parametrized by ''s'' ∈ '''Z''' (or '''R''', or '''N''' ∪ {0}, or [0, +∞)), where each transformation ''T<sub>s</sub>'' satisfies the same requirements as ''T'' above.<ref name=walters2000/> In particular, the transformations obey the rules:\n* <math>T_0 = \\mathrm{id}_X :X \\rightarrow X</math>, the [[identity function]] on ''X'';\n* <math>T_{s} \\circ T_{t} = T_{t + s}</math>, whenever all the terms are [[well-defined]];\n* <math>T_{s}^{-1} = T_{-s}</math>, whenever all the terms are well-defined.\n\nThe earlier, simpler case fits into this framework by defining ''T<sub>s</sub>'' = ''T<sup>s</sup>'' for ''s'' ∈ '''N'''.\n\n==Homomorphisms==\nThe concept of a [[homomorphism]] and an [[isomorphism]] may be defined.\n\nConsider two dynamical systems <math>(X, \\mathcal{A}, \\mu, T)</math> and <math>(Y, \\mathcal{B}, \\nu, S)</math>. Then a mapping\n\n:<math>\\varphi:X \\to Y</math>\n\nis a '''homomorphism of dynamical systems''' if it satisfies the following three properties:\n\n# The map <math>\\varphi\\ </math> is [[measurable function|measurable]].\n# For each <math>B \\in \\mathcal{B}</math>, one has <math>\\mu (\\varphi^{-1}B) = \\nu(B)</math>.\n# For [[almost everywhere|<math>\\mu</math>-almost all]] <math>x \\in X</math>, one has <math>\\varphi(Tx) = S(\\varphi x)</math>.\n\nThe system <math>(Y, \\mathcal{B}, \\nu, S)</math> is then called a '''factor''' of <math>(X, \\mathcal{A}, \\mu, T)</math>.\n\nThe map <math>\\varphi\\;</math> is an '''isomorphism of dynamical systems''' if, in addition, there exists another mapping\n\n:<math>\\psi:Y \\to X</math>\n\nthat is also a homomorphism, which satisfies\n\n# for <math>\\mu</math>-almost all <math>x \\in X</math>, one has <math>x = \\psi(\\varphi x)</math>;\n# for <math>\\nu</math>-almost all <math>y \\in Y</math>, one has <math>y = \\varphi(\\psi y)</math>.\n\nHence, one may form a [[category (mathematics)|category]] of dynamical systems and their homomorphisms.\n\n==Generic points==\nA point ''x'' ∈ ''X'' is called a '''generic point''' if the [[orbit (dynamics)|orbit]] of the point is [[ergodic theorem|distributed uniformly]] according to the measure.\n\n==Symbolic names and generators==\nConsider a dynamical system <math>(X, \\mathcal{B}, T, \\mu)</math>, and let ''Q'' = {''Q''<sub>1</sub>, ..., ''Q<sub>k</sub>''} be a [[partition of a set|partition]] of ''X'' into ''k'' measurable pair-wise disjoint sets.  Given a point ''x'' ∈ ''X'', clearly ''x'' belongs to only one of the ''Q<sub>i</sub>''.  Similarly, the iterated point ''T<sup>n</sup>x'' can belong to only one of the parts as well. The '''symbolic name''' of ''x'', with regards to the partition ''Q'', is the sequence of integers {''a''<sub>''n''</sub>} such that\n\n:<math>T^nx \\in Q_{a_n}.</math>\n\nThe set of symbolic names with respect to a partition is called the [[symbolic dynamics]] of the dynamical system.  A partition ''Q'' is called a '''generator''' or '''generating partition''' if μ-almost every point ''x'' has a unique symbolic name.\n\n==Operations on partitions==\nGiven a partition Q = {''Q''<sub>1</sub>, ..., ''Q''<sub>''k''</sub>} and a dynamical system <math>(X, \\mathcal{B}, T, \\mu)</math>, define the ''T''-pullback of ''Q'' as\n\n:<math> T^{-1}Q = \\{T^{-1}Q_1,\\ldots,T^{-1}Q_k\\}.</math>\n\nFurther, given two [[partition of a set|partitions]] ''Q'' = {''Q''<sub>1</sub>, ..., ''Q<sub>k</sub>''} and ''R'' = {''R''<sub>1</sub>, ..., ''R''<sub>''m''</sub>}, define their [[join (sigma algebra)|refinement]] as\n\n:<math> Q \\vee R = \\{Q_i \\cap R_j \\mid i=1,\\ldots,k,\\ j=1,\\ldots,m,\\ \\mu(Q_i \\cap R_j) > 0 \\}.</math>\n\nWith these two constructs, the ''refinement of an iterated pullback'' is defined as\n\n:<math>\n\\begin{align}\n\\bigvee_{n=0}^N T^{-n}Q & =  \\{Q_{i_0} \\cap T^{-1}Q_{i_1} \\cap \\cdots \\cap T^{-N}Q_{i_N} \\\\\n& {} \\qquad \\mbox { where }i_\\ell = 1,\\ldots,k ,\\ \\ell=0,\\ldots,N,\\ \\\\\n& {} \\qquad \\qquad \\mu \\left (Q_{i_0} \\cap T^{-1}Q_{i_1} \\cap \\cdots \\cap T^{-N}Q_{i_N} \\right )>0 \\} \\\\\n\\end{align}\n</math>\n\nwhich plays crucial role in the construction of the measure-theoretic entropy of a dynamical system.\n\n==Measure-theoretic entropy==\n{{see also|approximate entropy}}\nThe [[information entropy|entropy]] of a partition <math>\\mathcal{Q}</math> is defined as<ref>{{cite journal |first=Ya. G. |last=Sinai |year=1959 |title=On the Notion of Entropy of a Dynamical System |journal=[[Proceedings of the USSR Academy of Sciences|Doklady Akademii Nauk SSSR]] |volume=124 |pages=768–771 }}</ref><ref>{{cite web |first=Ya. G. |last=Sinai |year=2007 |url=https://web.math.princeton.edu/facultypapers/Sinai/MetricEntropy2.pdf |title=Metric Entropy of Dynamical System }}</ref>\n\n:<math>H(\\mathcal{Q})=-\\sum_{Q \\in \\mathcal{Q}}\\mu (Q) \\log \\mu(Q).</math>\n\nThe measure-theoretic entropy of a dynamical system <math>(X, \\mathcal{B}, T, \\mu)</math> with respect to a partition ''Q'' = {''Q''<sub>1</sub>, ..., ''Q''<sub>''k''</sub>} is then defined as\n\n:<math>h_\\mu(T,\\mathcal{Q}) = \\lim_{N \\rightarrow \\infty} \\frac{1}{N} H\\left(\\bigvee_{n=0}^N T^{-n}\\mathcal{Q}\\right).</math>\n\nFinally, the '''Kolmogorov–Sinai metric''' or '''measure-theoretic entropy''' of a dynamical system <math>(X, \\mathcal{B},T,\\mu)</math> is defined as\n\n:<math>h_\\mu(T) = \\sup_{\\mathcal{Q}} h_\\mu(T,\\mathcal{Q}).</math>\n\nwhere the [[supremum]] is taken over all finite measurable partitions. A theorem of [[Yakov Sinai]] in 1959 shows that the supremum is actually obtained on partitions that are generators.  Thus, for example, the entropy of the [[Bernoulli process]] is log&nbsp;2, since [[almost every]] [[real number]] has a unique [[binary expansion]]. That is, one may partition the [[unit interval]] into the intervals <nowiki>[</nowiki>0,&nbsp;1/2<nowiki>)</nowiki> and [1/2,&nbsp;1]. Every real number ''x'' is either less than 1/2 or not; and likewise so is the fractional part of 2<sup>''n''</sup>''x''.\n\nIf the space ''X'' is compact and endowed with a topology, or is a metric space, then the [[topological entropy]] may also be defined.\n\nIf <math>T</math> is an ergodic, piecewise expanding, and Markov on <math>X \\subset \\R</math>, and <math>\\mu</math> is absolutely continuous with respect to the Lebesgue measure, then we have the Rokhlin formula<ref>''[https://web.archive.org/web/20240117051216/https://www.mat.univie.ac.at/~bruin/ET1_lect15.pdf The Shannon-McMillan-Breiman Theorem]''</ref> (section 4.3 and section 12.3 <ref>{{Cite book |last=Pollicott |first=Mark |url=https://www.cambridge.org/core/books/dynamical-systems-and-ergodic-theory/3C1AA7BE85F5D2EE027D60CC72FDBEB8 |title=Dynamical Systems and Ergodic Theory |last2=Yuri |first2=Michiko |date=1998 |publisher=Cambridge University Press |isbn=978-0-521-57294-1 |series=London Mathematical Society Student Texts |location=Cambridge}}</ref>):<math display=\"block\">h_{\\mu }(T) = \\int \\ln |dT/dx| \\mu(dx) </math>This allows calculation of entropy of many interval maps, such as the [[logistic map]].\n\nErgodic means that <math>T^{-1}(A) = A</math> implies <math>A</math> has full measure or zero measure. Piecewise expanding and Markov means that there is a partition of <math>X</math> into finitely many open intervals, such that for some <math>\\epsilon > 0</math>, <math>|T'| \\geq 1 + \\epsilon</math> on each open interval. Markov means that for each <math>I_i</math> from those open intervals, either <math>T(I_i) \\cap I_i = \\emptyset </math> or <math>T(I_i) \\cap I_i = I_i </math>.\n\n==Classification and anti-classification theorems==\nOne of the primary activities in the study of measure-preserving systems is their classification according to their properties.  That is, let <math>(X, \\mathcal{B}, \\mu)</math> be a measure space, and let <math>U</math> be the set of all measure preserving systems <math>(X, \\mathcal{B}, \\mu, T)</math>. An isomorphism  <math>S\\sim T</math> of two transformations <math>S, T</math> defines an [[equivalence relation]] <math>\\mathcal{R}\\subset U\\times U.</math> The goal is then to describe the relation <math>\\mathcal{R}</math>. A number of classification theorems have been obtained; but quite interestingly, a number of anti-classification theorems have been found as well. The anti-classification theorems state that there are more than a countable number of isomorphism classes, and that a countable amount of information is not sufficient to classify isomorphisms.<ref>{{cite journal |first1=Matthew |last1=Foreman |first2=Benjamin |last2=Weiss |year=2019 |arxiv=1703.07093 |title=From Odometers to Circular Systems: A Global Structure Theorem |journal=Journal of Modern Dynamics |volume=15 |pages=345–423 |doi=10.3934/jmd.2019024\n|s2cid=119128525 }}</ref><ref>{{cite journal |first1=Matthew |last1=Foreman |first2=Benjamin |last2=Weiss |date=2022 |arxiv=1705.04414 |title=Measure preserving Diffeomorphisms of the Torus are unclassifiable |journal=[[Journal of the European Mathematical Society]] |volume=24 |issue=8 |pages=2605–2690 |doi=10.4171/JEMS/1151 |doi-access=free}}</ref>\n\nThe first anti-classification theorem, due to Hjorth, states that if <math>U</math> is endowed with the [[weak topology]], then the set <math>\\mathcal{R}</math> is not a [[Borel set]].<ref>{{cite journal |first=G. |last=Hjorth |year=2001 |title=On invariants for measure preserving transformations |journal=Fund. Math. |volume=169 |issue=1 |pages=51–84 |doi=10.4064/FM169-1-2 |s2cid=55619325 |doi-access=free }}</ref> There are a variety of other anti-classification results. For example, replacing isomorphism with [[Kakutani's theorem (measure theory)|Kakutani equivalence]], it can be shown that there are uncountably many non-Kakutani equivalent ergodic measure-preserving transformations of each entropy type.<ref>{{cite book |first1=D. |last1=Ornstein |authorlink1=Donald Samuel Ornstein |first2=D. |last2=Rudolph |first3=B. |last3=Weiss |year=1982 |title=Equivalence of measure preserving transformations |series=Mem. American Mathematical Soc. |volume=37 |issue=262 |isbn=0-8218-2262-4 }}</ref>\n\nThese stand in contrast to the classification theorems. These include:\n* Ergodic measure-preserving transformations with a pure point spectrum have been classified.<ref>{{cite journal |first1=P. |last1=Halmos |first2=J. |last2=von Neumann |year=1942 |title=Operator methods in classical mechanics. II. |journal=Annals of Mathematics |series=(2) |volume=43 |issue=2 |pages=332–350 |doi=10.2307/1968872 |jstor=1968872 }}</ref>\n* [[Bernoulli shift]]s are classified by their metric entropy.<ref>{{cite journal |first=Ya. |last=Sinai |year=1962 |title=A weak isomorphism of transformations with invariant measure |journal=[[Proceedings of the USSR Academy of Sciences|Doklady Akademii Nauk SSSR]] |volume=147 |pages=797–800 }}</ref><ref>{{cite journal |first=D. |last=Ornstein |authorlink=Donald Samuel Ornstein |year=1970 |title=Bernoulli shifts with the same entropy are isomorphic |journal=[[Advances in Mathematics]] |volume=4 |issue=3 |pages=337–352 |doi=10.1016/0001-8708(70)90029-0 |doi-access=free }}</ref><ref>{{cite book |first1=A. |last1=Katok |first2=B. |last2=Hasselblatt |year=1995 |chapter=Introduction to the modern theory of dynamical systems |title=Encyclopedia of Mathematics and its Applications |volume=54 |publisher=Cambridge University Press }}</ref> See [[Ornstein theory]] for more.\n\n{{Math theorem\n| math_statement = Given a dynamical system on a Lebesgue space of measure 1, where <math display=\"inline\">T</math> is invertible, measure preserving, and ergodic.\n\nIf <math>h_T \\leq \\ln k</math> for some integer <math>k</math>, then the system has a size-<math>k</math> generator.\n\nIf the entropy is exactly equal to <math>\\ln k</math>, then such a generator exists iff the system is isomorphic to the Bernoulli shift on <math>k</math> symbols with equal measures.\n| name = Krieger finite generator theorem<ref>{{Cite book |last=Downarowicz |first=Tomasz |title=Entropy in dynamical systems |date=2011 |publisher=Cambridge University Press |isbn=978-0-521-88885-1 |series=New Mathematical Monographs |location=Cambridge |page=106}}</ref>\n| note = Krieger 1970\n}}\n\n==See also==\n\n* {{annotated link|Krylov–Bogolyubov theorem}} on the existence of invariant measures\n* {{annotated link|Poincaré recurrence theorem}}\n\n==References==\n\n{{reflist}}\n\n==Further reading==\n\n* Michael S. Keane, \"Ergodic theory and subshifts of finite type\", (1991), appearing as Chapter 2 in ''Ergodic Theory, Symbolic Dynamics and Hyperbolic Spaces'', Tim Bedford, Michael Keane and Caroline Series, Eds. Oxford University Press, Oxford (1991). {{isbn|0-19-853390-X}} ''(Provides expository introduction, with exercises, and extensive references.)''\n* [[Lai-Sang Young]], \"Entropy in Dynamical Systems\" ([http://www.math.nyu.edu/~lsy/papers/entropy.pdf pdf]; [http://www.math.nyu.edu/~lsy/papers/entropy.ps ps]), appearing as Chapter 16 in ''Entropy'', Andreas Greven, Gerhard Keller, and Gerald Warnecke, eds. Princeton University Press, Princeton, NJ (2003).  {{isbn|0-691-11338-6}}\n* T. Schürmann and I. Hoffmann, ''The entropy of strange billiards inside n-simplexes.'' J. Phys. A 28(17), page 5033, 1995. [https://arxiv.org/abs/nlin/0208048 PDF-Document] ''(gives a more involved example of measure-preserving dynamical system.)''\n\n{{Measure theory}}\n\n[[Category:Dynamical systems]]\n[[Category:Entropy]]\n[[Category:Entropy and information]]\n[[Category:Information theory]]\n[[Category:Measure theory]]\n","x":-1718,"y":-347,"width":250,"height":60},
		{"id":"2be94c8c764b46cb","type":"text","text":"Homotopy equivalence\n---\n\n==Homotopy equivalence==\nGiven two topological spaces ''X'' and ''Y'', a '''homotopy equivalence''' between ''X'' and ''Y'' is a pair of continuous [[map (mathematics)|map]]s {{nowrap|1=''f'' : ''X'' → ''Y''}} and {{nowrap|1=''g'' : ''Y'' → ''X''}}, such that {{nowrap|1=''g''&thinsp;∘&thinsp;''f''}} is homotopic to the [[identity function|identity map]] id<sub>''X''</sub> and {{nowrap|1=''f''&thinsp;∘&thinsp;''g''}} is homotopic to id<sub>''Y''</sub>. If such a pair exists, then ''X'' and ''Y'' are said to be '''homotopy equivalent''', or of the same '''homotopy type'''. This relation of homotopy equivalence is often denoted <math>\\simeq</math>.<ref>{{cite book\n | last = Singh | first = Tej Bahadur\n | doi = 10.1007/978-981-13-6954-4\n | isbn = 9789811369544\n | page = 317\n | publisher = Springer Singapore\n | title = Introduction to Topology\n | year = 2019}} This is the misnamed [[unicode]] symbol {{unichar|2243}}.</ref> Intuitively, two spaces ''X'' and ''Y'' are homotopy equivalent if they can be transformed into one another by bending, shrinking and expanding operations. Spaces that are homotopy-equivalent to a point are called [[contractible]].\n\n=== Homotopy equivalence vs. homeomorphism ===\nA [[homeomorphism]] is a special case of a homotopy equivalence, in which {{nowrap|1=''g''&thinsp;∘&thinsp;''f''}} is equal to the identity map id<sub>''X''</sub> (not only homotopic to it), and {{nowrap|1=''f''&thinsp;∘&thinsp;''g''}} is equal to id<sub>''Y''</sub>.<ref>Archived at [https://ghostarchive.org/varchive/youtube/20211211/XxFGokyYo6g Ghostarchive]{{cbignore}} and the [https://web.archive.org/web/20200829013025/https://www.youtube.com/watch?v=XxFGokyYo6g&gl=US&hl=en Wayback Machine]{{cbignore}}: {{Cite web|last=Albin|first=Pierre|date=2019|title=History of algebraic topology|website=[[YouTube]] |url=https://www.youtube.com/watch?v=XxFGokyYo6g}}{{cbignore}}</ref>{{Rp|0:53:00}} Therefore, if X and Y are homeomorphic then they are homotopy-equivalent, but the opposite is not true. Some examples:\n\n* A solid disk is homotopy-equivalent to a single point, since you can deform the disk along radial lines continuously to a single point. However, they are not homeomorphic, since there is no [[bijection]] between them (since one is an infinite set, while the other is finite).\n* The [[Möbius strip]] and an untwisted (closed) strip are homotopy equivalent, since you can deform both strips continuously to a circle. But they are not homeomorphic.\n\n=== Examples ===\n* The first example of a homotopy equivalence is <math>\\mathbb{R}^n</math> with a point, denoted <math>\\mathbb{R}^n \\simeq \\{ 0\\}</math>. The part that needs to be checked is the existence of a homotopy <math>H: I \\times \\mathbb{R}^n \\to \\mathbb{R}^n</math> between <math>\\operatorname{id}_{\\mathbb{R}^n}</math> and <math>p_0</math>, the projection of <math>\\mathbb{R}^n</math> onto the origin. This can be described as <math>H(t,\\cdot) = t\\cdot p_0 + (1-t)\\cdot\\operatorname{id}_{\\mathbb{R}^n}</math>.\n* There is a homotopy equivalence between <math>S^1</math> (the [[n-sphere|1-sphere]]) and <math>\\mathbb{R}^2-\\{0\\}</math>.\n** More generally, <math>\\mathbb{R}^n-\\{ 0\\} \\simeq S^{n-1}</math>.\n* Any [[fiber bundle]] <math>\\pi: E \\to B</math> with fibers <math>F_b</math> homotopy equivalent to a point has homotopy equivalent total and base spaces. This generalizes the previous two examples since <math>\\pi:\\mathbb{R}^n - \\{0\\} \\to S^{n-1}</math> is a fiber bundle with fiber <math>\\mathbb{R}_{>0}</math>.\n* Every [[vector bundle]] is a fiber bundle with a fiber homotopy equivalent to a point.\n* <math>\\mathbb{R}^n - \\mathbb{R}^k \\simeq S^{n-k-1}</math> for any <math>0 \\le k < n</math>, by writing <math>\\mathbb{R}^n - \\mathbb{R}^k</math> as the total space of the fiber bundle <math>\\mathbb{R}^k \\times (\\mathbb{R}^{n-k}-\\{0\\})\\to (\\mathbb{R}^{n-k}-\\{0\\})</math>, then applying the homotopy equivalences above.\n* If a subcomplex <math>A</math> of a [[CW complex]] <math>X</math> is contractible, then the [[quotient space (topology)|quotient space]] <math>X/A</math> is homotopy equivalent to <math>X</math>.<ref>{{Cite book|title=Algebraic topology|last=Allen.|first=Hatcher|date=2002|publisher=Cambridge University Press|isbn=9780521795401|location=Cambridge|pages=11|oclc=45420394}}</ref>\n* A [[deformation retraction]] is a homotopy equivalence.\n\n===Null-homotopy===\nA function <math>f</math> is said to be '''null-homotopic''' {{anchor|null homotopic}} if it is homotopic to a constant function.  (The homotopy from <math>f</math> to a constant function is then sometimes called a '''null-homotopy'''.)  For example, a map <math>f</math> from the [[unit circle]] <math>S^1</math> to any space <math>X</math> is null-homotopic precisely when it can be continuously extended to a map from the [[unit disk]] <math>D^2</math> to <math>X</math> that agrees with <math>f</math> on the boundary.\n\nIt follows from these definitions that a space <math>X</math> is contractible if and only if the identity map from <math>X</math> to itself&mdash;which is always a homotopy equivalence&mdash;is null-homotopic.\n","x":453,"y":-1047,"width":250,"height":487},
		{"id":"2fca25f21f48fa6e","type":"text","text":"=== Examples ===\n* The first example of a homotopy equivalence is <math>\\mathbb{R}^n</math> with a point, denoted <math>\\mathbb{R}^n \\simeq \\{ 0\\}</math>. The part that needs to be checked is the existence of a homotopy <math>H: I \\times \\mathbb{R}^n \\to \\mathbb{R}^n</math> between <math>\\operatorname{id}_{\\mathbb{R}^n}</math> and <math>p_0</math>, the projection of <math>\\mathbb{R}^n</math> onto the origin. This can be described as <math>H(t,\\cdot) = t\\cdot p_0 + (1-t)\\cdot\\operatorname{id}_{\\mathbb{R}^n}</math>.\n* There is a homotopy equivalence between <math>S^1</math> (the [[n-sphere|1-sphere]]) and <math>\\mathbb{R}^2-\\{0\\}</math>.\n** More generally, <math>\\mathbb{R}^n-\\{ 0\\} \\simeq S^{n-1}</math>.\n* Any [[fiber bundle]] <math>\\pi: E \\to B</math> with fibers <math>F_b</math> homotopy equivalent to a point has homotopy equivalent total and base spaces. This generalizes the previous two examples since <math>\\pi:\\mathbb{R}^n - \\{0\\} \\to S^{n-1}</math> is a fiber bundle with fiber <math>\\mathbb{R}_{>0}</math>.\n* Every [[vector bundle]] is a fiber bundle with a fiber homotopy equivalent to a point.\n* <math>\\mathbb{R}^n - \\mathbb{R}^k \\simeq S^{n-k-1}</math> for any <math>0 \\le k < n</math>, by writing <math>\\mathbb{R}^n - \\mathbb{R}^k</math> as the total space of the fiber bundle <math>\\mathbb{R}^k \\times (\\mathbb{R}^{n-k}-\\{0\\})\\to (\\mathbb{R}^{n-k}-\\{0\\})</math>, then applying the homotopy equivalences above.\n* If a subcomplex <math>A</math> of a [[CW complex]] <math>X</math> is contractible, then the [[quotient space (topology)|quotient space]] <math>X/A</math> is homotopy equivalent to <math>X</math>.<ref>{{Cite book|title=Algebraic topology|last=Allen.|first=Hatcher|date=2002|publisher=Cambridge University Press|isbn=9780521795401|location=Cambridge|pages=11|oclc=45420394}}</ref>\n* A [[deformation retraction]] is a homotopy equivalence.\n","x":715,"y":-1000,"width":250,"height":260},
		{"id":"b081c8947ebe3d94","type":"text","text":"Invariance\n---\n==Invariance==\nHomotopy equivalence is important because in [[algebraic topology]] many concepts are '''homotopy invariant''', that is, they respect the relation of homotopy equivalence.  For example, if ''X'' and ''Y'' are homotopy equivalent spaces, then:\n* ''X'' is [[connected space|path-connected]] if and only if ''Y'' is.\n* ''X'' is [[simply connected]] if and only if ''Y'' is.\n* The (singular) [[homology (mathematics)|homology]] and [[cohomology group]]s of ''X'' and ''Y'' are [[group isomorphism|isomorphic]].\n* If ''X'' and ''Y'' are path-connected, then the [[fundamental group]]s of ''X'' and ''Y'' are isomorphic, and so are the higher [[homotopy group]]s. (Without the path-connectedness assumption, one has π<sub>1</sub>(''X'',&thinsp;''x''<sub>0</sub>) isomorphic to π<sub>1</sub>(''Y'', ''f''(''x''<sub>0</sub>)) where {{nowrap|1=''f'' : ''X'' → ''Y''}} is a homotopy equivalence and {{nowrap|1=''x''<sub>0</sub> &isin; ''X''.)}}\n\nAn example of an algebraic invariant of topological spaces which is not homotopy-invariant is [[compactly supported homology]] (which is, roughly speaking, the homology of the [[compactification (mathematics)|compactification]], and compactification is not homotopy-invariant).\n","x":760,"y":-700,"width":250,"height":215},
		{"id":"150acd63fbd61246","type":"text","text":"Relative homotopy - **pointed homotopy**\n---\n===Relative homotopy===\n\nIn order to define the [[fundamental group]], one needs the notion of '''homotopy relative to a subspace'''. These are homotopies which keep the elements of the subspace fixed. Formally: if ''f'' and ''g'' are continuous maps from ''X'' to ''Y'' and ''K'' is a [[subset]] of ''X'', then we say that ''f'' and ''g'' are homotopic relative to ''K'' if there exists a homotopy {{nowrap|1=''H'' : ''X'' &times; [0,&thinsp;1] → ''Y''}} between ''f'' and ''g'' such that {{nowrap|1=''H''(''k'',&thinsp;''t'') = ''f''(''k'') = ''g''(''k'')}} for all {{nowrap|1=''k'' ∈ ''K''}} and {{nowrap|1=''t'' ∈ [0,&thinsp;1].}} Also, if ''g'' is a [[retraction (topology)|retraction]] from ''X'' to ''K'' and ''f'' is the identity map, this is known as a strong [[deformation retract]] of ''X'' to ''K''.\nWhen ''K'' is a point, the term '''pointed homotopy''' is used.\n","x":974,"y":-969,"width":326,"height":229},
		{"id":"ebb4753de9fb524b","type":"text","text":"=== Isotopy ===\n---\n\n{{multiple image\n| total_width = 320\n| image1 = Blue Unknot.png\n| image2 = Blue Trefoil Knot.png\n| footer = The [[unknot]] is not equivalent to the [[trefoil knot]] since one cannot be deformed into the other through a continuous path of homeomorphisms of the ambient space. Thus they are not ambient-isotopic.\n}}\nWhen two given continuous functions ''f'' and ''g'' from the topological space ''X'' to the topological space ''Y'' are [[embedding]]s, one can ask whether they can be connected 'through embeddings'. This gives rise to the concept of '''isotopy''', which is a homotopy, ''H'', in the notation used before, such that for each fixed ''t'', ''H''(''x'',&thinsp;''t'') gives an embedding.<ref>{{MathWorld|Isotopy|Isotopy}}</ref>\n\nA related, but different, concept is that of [[ambient isotopy]].\n\nRequiring that two embeddings be isotopic is a stronger requirement than that they be homotopic. For example, the map from the interval [−1,&thinsp;1] into the real numbers defined by ''f''(''x'') = &minus;''x'' is ''not'' isotopic to the identity ''g''(''x'') = ''x''.  Any homotopy from ''f'' to the identity would have to exchange the endpoints, which would mean that they would have to 'pass through' each other. Moreover, ''f'' has changed the orientation of the interval and ''g'' has not, which is impossible under an isotopy. However, the maps are homotopic; one homotopy from ''f'' to the identity is ''H'':&nbsp;[−1,&thinsp;1]&nbsp;&times;&nbsp;[0,&thinsp;1]&nbsp;→&nbsp;[−1,&thinsp;1] given by ''H''(''x'',&thinsp;''y'')&nbsp;=&nbsp;2''yx''&nbsp;−&nbsp;''x''.\n\nTwo homeomorphisms (which are special cases of embeddings) of the unit ball which agree on the boundary can be shown to be isotopic using [[Alexander's trick]]. For this reason, the map of the [[unit disc]] in <math>\\mathbb{R}^2</math> defined by ''f''(''x'',&thinsp;''y'') = (&minus;''x'',&nbsp;&minus;''y'') is isotopic to a 180-degree [[rotation]] around the origin, and so the identity map and ''f'' are isotopic because they can be connected by rotations.\n\nIn [[geometric topology]]&mdash;for example in [[knot theory]]&mdash;the idea of isotopy is used to construct equivalence relations. For example, when should two knots be considered the same?  We take two knots, ''K''<sub>1</sub> and ''K''<sub>2</sub>, in three-[[dimension]]al space. A knot is an [[embedding]] of a one-dimensional space, the \"loop of string\" (or the circle), into this space, and this embedding gives a homeomorphism between the circle and its image in the embedding space. One may try to define knot equivalence based on isotopy instead of the more restricted property of [[ambient isotopy]]. That is, two knots are isotopic when there exists a continuous function starting at ''t''&nbsp;=&nbsp;0 giving the ''K''<sub>1</sub> embedding, ending at ''t''&nbsp;=&thinsp;1 giving the ''K''<sub>2</sub> embedding, with all intermediate values corresponding to embeddings. However, this definition would make every knot equivalent to the unknot, as the knotted portions can be \"contracted\" down to a straight line. The problem is that, while continuous, this is not an injective function of the euclidean space that the knot is embedded in. An [[ambient isotopy]], studied in this context, is an isotopy of the larger space, considered in light of its action on the embedded submanifold. Knots ''K''<sub>1</sub> and ''K''<sub>2</sub> are considered equivalent when there is a continuous which moves ''K''<sub>1</sub> to ''K''<sub>2</sub> via homeomorphisms of the euclidean space. \n\nSimilar language is used for the equivalent concept in contexts where one has a stronger notion of equivalence.  For example, a path between two smooth embeddings is a '''smooth isotopy'''.\n","x":1042,"y":-669,"width":338,"height":209},
		{"id":"12bbbeeafda8dd5c","type":"text","text":" Homotopy group\n ---\n ===Groups===\n\n{{main|Homotopy group}}\nSince the relation of two functions <math>f, g\\colon X\\to Y</math> being homotopic relative to a subspace is an equivalence relation, we can look at the [[equivalence class]]es of maps between a fixed ''X'' and ''Y''. If we fix <math>X = [0,1]^n</math>, the unit interval [0,&thinsp;1] [[cartesian product|crossed]] with itself ''n'' times, and we take its [[Boundary (topology)|boundary]] <math>\\partial([0,1]^n)</math> as a subspace, then the equivalence classes form a group, denoted <math>\\pi_n(Y,y_0)</math>, where <math>y_0</math> is in the image of the subspace <math>\\partial([0,1]^n)</math>.\n\nWe can define the action of one equivalence class on another, and so we get a group. These groups are called the [[homotopy group]]s. In the case <math>n = 1</math>, it is also called the [[fundamental group]].\n","x":1320,"y":-969,"width":323,"height":199},
		{"id":"403d88886a1387cf","type":"text","text":"Homotopy category\n---\n\n===Homotopy category===\n{{Main|Homotopy category}}\nThe idea of homotopy can be turned into a formal category of [[category theory]]. The '''[[homotopy category]]''' is the category whose objects are topological spaces, and whose morphisms are homotopy equivalence classes of continuous maps. Two topological spaces ''X'' and ''Y'' are isomorphic in this category if and only if they are homotopy-equivalent.  Then a [[functor]] on the category of topological spaces is homotopy invariant if it can be expressed as a functor on the homotopy category.\n\nFor example, homology groups are a ''functorial'' homotopy invariant: this means that if ''f'' and ''g'' from ''X'' to ''Y'' are homotopic, then the [[group homomorphism]]s induced by ''f'' and ''g'' on the level of [[homology group]]s are the same: H<sub>''n''</sub>(''f'') = H<sub>''n''</sub>(''g'') : H<sub>''n''</sub>(''X'') → H<sub>''n''</sub>(''Y'') for all ''n''.  Likewise, if ''X'' and ''Y'' are in addition [[connectedness|path connected]], and the homotopy between ''f'' and ''g'' is pointed, then the group homomorphisms induced by ''f'' and ''g'' on the level of [[homotopy group]]s are also the same: π<sub>''n''</sub>(''f'') = π<sub>''n''</sub>(''g'') : π<sub>''n''</sub>(''X'') → π<sub>''n''</sub>(''Y'').\n","x":1394,"y":-708,"width":250,"height":60},
		{"id":"36bb034570983d55","type":"text","text":"Homology of chain complexes\n---\n== Homology of chain complexes ==\nTo take the homology of a [[chain complex]], one starts with a '''chain complex,''' which is a sequence <math>  (C_\\bullet, d_\\bullet)</math> of [[Abelian group|abelian groups]] <math>C_{n}</math> (whose elements are called [[Chain (algebraic topology)|chains]]) and [[Group homomorphism|group homomorphisms]] <math>d_n</math> (called [[Boundary map|boundary maps]]) such that the composition of any two consecutive [[Map (mathematics)|maps]] is zero:\n: <math> C_\\bullet: \\cdots \\longrightarrow \nC_{n+1} \\stackrel{d_{n+1}}{\\longrightarrow}\nC_n \\stackrel{d_n}{\\longrightarrow}\nC_{n-1} \\stackrel{d_{n-1}}{\\longrightarrow}\n\\cdots, \\quad d_n \\circ d_{n+1}=0.</math><!--''d''<sub>''n''+1</sub> o ''d''<sub>''n''</sub> = 0 for all ''n''.-->\n\nThe <math>n</math>th homology group <math>H_{n}</math> of this chain complex is then the [[quotient group]] <math>H_n = Z_n/B_n</math> of cycles [[Quotient group|modulo]] boundaries, where the <math>n\n</math>th group of '''cycles''' <math>Z_n</math> is given by the [[Kernel (algebra)|kernel]] subgroup <math>Z_n := \\ker d_n :=\\{c \\in C_n \\,|\\; d_n(c) = 0\\}</math>, and the <math>n</math>th group of '''boundaries''' <math>B_n</math> is given by the [[Image (mathematics)|image]] subgroup <math>B_n := \\mathrm{im}\\, d_{n+1} :=\\{d_{n+1}(c)\\,|\\; c\\in C_{n+1}\\}</math>. One can optionally endow chain complexes with additional structure, for example by additionally taking the groups <math>C_n</math> to be [[Module (mathematics)|modules]] over a [[Ring (mathematics)|coefficient ring]] <math>R</math>, and taking the boundary maps <math>d_n</math> to be <math>R</math>-[[Module homomorphism|module homomorphisms]], resulting in homology groups <math>H_{n}</math> that are also [[Quotient module|quotient modules]]. Tools from [[homological algebra]] can be used to relate homology groups of different chain complexes.\n","x":693,"y":-382,"width":250,"height":60},
		{"id":"07062e5e8b25dac2","type":"text","text":"Homology functors\n---\n== Homology functors ==\nChain complexes form a [[category (mathematics)|category]]: A morphism from the chain complex (<math>d_n : A_n \\to A_{n-1}</math>) to the chain complex (<math>e_n : B_n \\to B_{n-1}</math>) is a sequence of homomorphisms <math>f_n : A_n \\to B_n</math> such that <math>f_{n-1} \\circ d_n = e_n \\circ f_n</math> for all ''n''. The ''n''-th homology ''H<sub>n</sub>'' can be viewed as a covariant [[functor]] from the category of chain complexes to the category of abelian groups (or modules).\n\nIf the chain complex depends on the object ''X'' in a covariant manner (meaning that any morphism <math>X \\to Y</math> induces a morphism from the chain complex of  ''X''  to the chain complex of ''Y''), then the ''H<sub>n</sub>'' are covariant [[functor]]s from the category that ''X'' belongs to into the category of abelian groups (or modules).\n\nThe only difference between homology and [[cohomology]] is that in cohomology the chain complexes depend in a ''contravariant'' manner on ''X'', and that therefore the homology groups (which are called ''cohomology groups'' in this context and denoted by ''H<sup>n</sup>'') form ''contravariant'' functors from the category that ''X'' belongs to into the category of abelian groups or modules.\n","x":994,"y":-410,"width":250,"height":60},
		{"id":"0d00ef43b49cdde2","type":"text","text":"# K-homology\n---\nIn [[mathematics]], '''K-homology''' is a [[homology (mathematics)|homology]] theory on the [[Category (mathematics)|category]] of [[locally compact]] [[Hausdorff space]]s. It classifies the elliptic [[pseudo-differential operator]]s acting on the [[vector bundle]]s over a space. In terms of [[C*-algebra|<math>C^*</math>-algebras]], it classifies the [[Fredholm module]]s over an [[algebra]]. \n\nAn '''operator [[homotopy]]''' between two Fredholm modules  <math>(\\mathcal{H},F_0,\\Gamma)</math> and  <math>(\\mathcal{H},F_1,\\Gamma)</math> is a [[norm (mathematics)|norm]] [[Continuous function|continuous]] [[Path (topology)|path]] of Fredholm modules,  <math>t \\mapsto (\\mathcal{H},F_t,\\Gamma)</math>,  <math>t \\in [0,1].</math> Two Fredholm modules are then equivalent if they are related by [[unitary transformation]]s or operator homotopies. The <math>K^0(A)</math> [[group (mathematics)|group]] is the [[abelian group]] of [[equivalence relation|equivalence classes]] of even Fredholm modules over A. The <math>K^1(A)</math> group is the abelian group of equivalence classes of odd Fredholm modules over A. Addition is given by [[Direct sum of modules|direct summation]] of Fredholm modules, and the [[Inverse function|inverse]] of  <math>(\\mathcal{H}, F, \\Gamma)</math> is  <math>(\\mathcal{H}, -F, -\\Gamma).</math>\n\n== References ==\n* N. Higson and J. Roe, ''Analytic K-homology''. Oxford University Press, 2000.\n\n{{PlanetMath attribution|id=3330|title=K-homology}}\n\n[[Category:K-theory]]\n[[Category:Homology theory]]\n\n---\n","x":1150,"y":-291,"width":250,"height":311},
		{"id":"74b20ef3840ac106","type":"text","text":"== Construction of homology groups ==\nThe following text describes a general algorithm for constructing the homology groups. It may be easier for the reader to look at some simple examples first: [[graph homology]] and [[simplicial homology]].\n\nThe general construction begins with an object such as a topological space ''X'', on which one first defines a {{em|[[chain complex]]}} ''C''(''X'') encoding information about ''X''. A chain complex is a sequence of abelian groups or modules <math>C_0, C_1, C_2, \\ldots</math>. connected by [[group homomorphism|homomorphisms]] <math>\\partial_n : C_n \\to C_{n-1},</math> which are called '''boundary operators'''.<ref name=\"Hatcher 2002 106\" /> That is,\n: <math>\n\\dotsb\n\\overset{\\partial_{n+1}}{\\longrightarrow\\,} C_n\n\\overset{\\partial_n}{\\longrightarrow\\,} C_{n-1}\n\\overset{\\partial_{n-1}}{\\longrightarrow\\,} \\dotsb\n\\overset{\\partial_2}{\\longrightarrow\\,} C_1\n\\overset{\\partial_1}{\\longrightarrow\\,} C_0\n\\overset{\\partial_0}{\\longrightarrow\\,} 0\n</math>\nwhere 0 denotes the trivial group and <math>C_i\\equiv0</math> for ''i'' < 0. It is also required that the composition of any two consecutive boundary operators be trivial. That is, for all ''n'',\n: <math>\\partial_n \\circ \\partial_{n+1} = 0_{n+1, n-1},</math>\ni.e., the constant map sending every element of <math>C_{n+1}</math> to the group identity in <math>C_{n-1}.</math>\n\nThe statement that the boundary of a boundary is trivial is equivalent to the statement that <math>\\mathrm{im}(\\partial_{n+1})\\subseteq\\ker(\\partial_n)</math>, where <math>\\mathrm{im}(\\partial_{n+1})</math> denotes the [[image (mathematics)|image]] of the boundary operator and <math>\\ker(\\partial_n)</math> its [[kernel (algebra)|kernel]]. Elements of <math>B_n(X) = \\mathrm{im}(\\partial_{n+1})</math> are called '''boundaries''' and elements of <math>Z_n(X) = \\ker(\\partial_n)</math> are called '''cycles'''.\n\nSince each chain group ''C<sub>n</sub>'' is abelian all its subgroups are normal. Then because <math>\\ker(\\partial_n)</math> is a subgroup of ''C<sub>n</sub>'', <math>\\ker(\\partial_n)</math> is abelian, and since <math>\\mathrm{im}(\\partial_{n+1}) \\subseteq\\ker(\\partial_n)</math> therefore <math>\\mathrm{im}(\\partial_{n+1})</math> is a [[normal subgroup]] of <math>\\ker(\\partial_n)</math>. Then one can create the [[quotient group]]\n: <math>H_n(X) := \\ker(\\partial_n) / \\mathrm{im}(\\partial_{n+1}) = Z_n(X)/B_n(X),</math>\ncalled the '''''n''th homology group of ''X'''''. The elements of ''H<sub>n</sub>''(''X'') are called '''homology classes'''. Each homology class is an equivalence class over cycles and two cycles in the same homology class are said to be '''homologous'''.<ref>{{Harvnb|Hatcher|2002|pp=105–106}}</ref>\n\nA chain complex is said to be [[exact sequence|exact]] if the image of the (''n''+1)th map is always equal to the kernel of the ''n''th map. The homology groups of ''X'' therefore measure \"how far\" the chain complex associated to ''X'' is from being exact.<ref>{{Harvnb|Hatcher|2002|p=113}}</ref>\n\nThe [[Reduced homology|reduced homology groups]] of a chain complex ''C''(''X'') are defined as homologies of the augmented chain complex<ref>{{Harvnb|Hatcher|2002|p=110}}</ref>\n: <math>\n\\dotsb\n\\overset{\\partial_{n+1}}{\\longrightarrow\\,} C_n\n\\overset{\\partial_n}{\\longrightarrow\\,} C_{n-1}\n\\overset{\\partial_{n-1}}{\\longrightarrow\\,} \\dotsb\n\\overset{\\partial_2}{\\longrightarrow\\,} C_1\n\\overset{\\partial_1}{\\longrightarrow\\,} C_0\n\\overset{\\epsilon}{\\longrightarrow\\,} \\Z\n{\\longrightarrow\\,} 0\n</math>\nwhere the boundary operator <math>\\epsilon</math> is\n: <math>\\epsilon \\left(\\sum_i n_i \\sigma_i\\right) = \\sum_i n_i</math>\nfor a combination <math>\\sum n_i \\sigma_i,</math> of points <math>\\sigma_i,</math> which are the fixed generators of ''C''<sub>0</sub>. The reduced homology groups <math>\\tilde{H}_i(X)</math> coincide with <math>H_i(X)</math> for <math>i \\neq 0.</math> The extra <math>\\Z</math> in the chain complex represents the unique map <math>[\\emptyset] \\longrightarrow X</math> from the empty simplex to ''X''.\n\nComputing the cycle <math>Z_n(X)</math> and boundary <math>B_n(X)</math> groups is usually rather difficult since they have a very large number of generators. On the other hand, there are tools which make the task easier.\n\nThe ''[[simplicial homology]]'' groups ''H<sub>n</sub>''(''X'') of a ''[[simplicial complex]]'' ''X'' are defined using the simplicial chain complex ''C''(''X''), with ''C<sub>n</sub>''(''X'') the [[free abelian group]] generated by the ''n''-simplices of ''X''. See [[simplicial homology]] for details.\n\nThe ''[[singular homology]]'' groups ''H<sub>n</sub>''(''X'') are defined for any topological space ''X'', and agree with the simplicial homology groups for a simplicial complex.\n\nCohomology groups are formally similar to homology groups: one starts with a [[cochain complex]], which is the same as a chain complex but whose arrows, now denoted <math>d_n,</math> point in the direction of increasing ''n'' rather than decreasing ''n''; then the groups <math>\\ker\\left(d^n\\right) = Z^n(X)</math> of ''cocycles'' and <math>\\mathrm{im}\\left(d^{n-1}\\right) = B^n(X)</math> of {{em|coboundaries}} follow from the same description. The ''n''th cohomology group of ''X'' is then the quotient group\n: <math>H^n(X) = Z^n(X)/B^n(X),</math>\nin analogy with the ''n''th homology group.\n","x":200,"y":-291,"width":365,"height":316},
		{"id":"846a593d2554abf4","type":"text","text":"Computations of algebraic K-theory\n---\n==Computations of algebraic K-theory==\n\nThe [[cyclotomic trace map]] is a map from [[algebraic K-theory]] (of a ring ''A'', say), to cyclic homology:\n\n:<math>tr: K_n (A) \\to HC_{n-1} (A).</math>\n\nIn some situations, this map can be used to compute K-theory by means of this map. A pioneering result in this direction is a theorem of {{harvtxt|Goodwillie|1986}}: it asserts that the map\n\n:<math>K_n(A, I) \\otimes \\mathbf Q \\to HC_{n-1} (A, I) \\otimes \\mathbf Q </math>\n\nbetween the relative K-theory of ''A'' with respect to a ''nilpotent'' two-sided ideal ''I'' to the relative cyclic homology (measuring the difference between K-theory or cyclic homology of ''A'' and of ''A''/''I'') is an isomorphism for ''n''&ge;1.\n\nWhile Goodwillie's result holds for arbitrary rings, a quick reduction shows that it is in essence only a statement about <math>A \\otimes_{\\mathbf Z} \\mathbf Q</math>. For rings not containing '''Q''', cyclic homology must be replaced by topological cyclic homology in order to keep a close connection to K-theory. (If '''Q''' is contained in ''A'', then cyclic homology and topological cyclic homology of ''A'' agree.) This is in line with the fact that (classical) [[Hochschild homology]] is less well-behaved than topological Hochschild homology for rings not containing '''Q'''. {{harvtxt|Clausen|Mathew|Morrow|2018}} proved a far-reaching generalization of Goodwillie's result, stating that for a commutative ring ''A'' so that the [[Henselian ring|Henselian lemma]] holds with respect to the ideal ''I'', the relative K-theory is isomorphic to relative topological cyclic homology (without tensoring both with '''Q'''). Their result also encompasses a theorem of {{harvtxt|Gabber|1992}}, asserting that in this situation the relative K-theory spectrum modulo an integer ''n'' which is invertible in ''A'' vanishes. {{harvtxt|Jardine|1993}} used Gabber's result and [[Suslin rigidity]] to reprove Quillen's computation of the K-theory of [[finite field]]s.\n","x":786,"y":-244,"width":250,"height":198},
		{"id":"87a07e9ecd9e2e45","x":486,"y":341,"width":494,"height":359,"type":"text","text":"Any f\n\nAfter we choose an item, we put it back, so we might choose it again.\n\nInjective f\n\nAfter we choose an item, we set it aside, so we can't choose it again; hence we'll end up with n distinct items. Necessarily, then, unless n≤x![{\\displaystyle n\\leq x}](https://wikimedia.org/api/rest_v1/media/math/render/svg/629e475fed09acb4b4b49c94d9ea781619ade867), no lists can be chosen at all.\n\nSurjective f\n\nAfter we choose an item, we put it back, so we might choose it again — but at the end, we have to end up having chosen each item at least once. Necessarily, then, unless n≥x![{\\displaystyle n\\geq x}](https://wikimedia.org/api/rest_v1/media/math/render/svg/45e1741be298822a096da450d73533ef4ca11aec), no lists can be chosen at all."}
	],
	"edges":[
		{"id":"75195233be62dd5c","fromNode":"385328f9840bd8a3","fromSide":"bottom","toNode":"6983a6011f062d9f","toSide":"top"},
		{"id":"1bf60652eb0153a5","fromNode":"f9275a4ea429aca1","fromSide":"top","toNode":"cbd7a7360adcfa98","toSide":"bottom","label":"undefined(-0)"},
		{"id":"83e6e418773bd6a4","fromNode":"f9275a4ea429aca1","fromSide":"top","toNode":"64c24f74c8c0740b","toSide":"bottom","label":"coefficent predicate "},
		{"id":"c1ae157645431a72","fromNode":"3a954dae48f9f2b9","fromSide":"top","toNode":"f9275a4ea429aca1","toSide":"bottom"},
		{"id":"bc5a570a0c4ddde6","fromNode":"e668ac1d4c914e71","fromSide":"top","toNode":"f9275a4ea429aca1","toSide":"bottom"},
		{"id":"1f9b08cc319dbd86","fromNode":"1fc5bf82d0f7ef54","fromSide":"right","toNode":"f9275a4ea429aca1","toSide":"left"},
		{"id":"768890c78f21d78b","fromNode":"acc014afcd745a1d","fromSide":"right","toNode":"1fc5bf82d0f7ef54","toSide":"left"},
		{"id":"dd4973c4777c73f5","fromNode":"acc014afcd745a1d","fromSide":"right","toNode":"cf4dae9e161473eb","toSide":"left"},
		{"id":"e467569c96200360","fromNode":"cf4dae9e161473eb","fromSide":"right","toNode":"f9275a4ea429aca1","toSide":"left"}
	]
}